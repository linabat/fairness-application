{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaab8ce-5433-4840-9aa8-c33499ba6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fairlearn\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9af067-aa00-404e-9bbb-9fa55fd4367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Custom Gradient Reversal Layer\n",
    "# -------------------------------\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x, lambda_):\n",
    "    def grad(dy):\n",
    "        return -lambda_ * dy, None # reverses direction of gradient \n",
    "    return x, grad\n",
    "\n",
    "# custom Keras layer\n",
    "\"\"\"\n",
    "Layer is used to ensure that the feature representation are independent of a sensitive attribute\n",
    "- feature extract learns normally in the forward pass\n",
    "- reversing gradients of classifier that tries to predict the sensitive attribute during backpropagation -- stops feature extractor from encoding sensitive information\n",
    "\"\"\"\n",
    "class GradientReversalLayer(tf.keras.layers.Layer): \n",
    "    def __init__(self, lambda_=1.0, **kwargs):\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "        self.lambda_ = lambda_ # strength of gradient reversal\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x, self.lambda_)\n",
    "\n",
    "# -------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# -------------------------------\n",
    "def set_seed(seed_num):\n",
    "    random.seed(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a279a6-a67d-4bab-8049-18a4eb1ab4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Adversarial Debiasing Model\n",
    "# -------------------------------\n",
    "def build_adversarial_model(input_dim, lambda_adv=1.0):\n",
    "    \"\"\"\n",
    "    Build an adversarial debiasing model that learns pseudo‑labels Y' from X.\n",
    "\n",
    "    Architecture:\n",
    "      - Main branch (encoder): from X, several dense layers produce a latent pseudo‑label pseudo_Y (via sigmoid).\n",
    "      - Adversary branch: pseudo_Y is passed through a Gradient Reversal Layer and then dense layers predict S.\n",
    "      - Decoder branch: concatenates pseudo_Y and the one-hot sensitive attribute S to predict the observed label Y.\n",
    "\n",
    "    Losses:\n",
    "      - For the main branch, binary crossentropy between observed Y and pseudo_Y (and Y_pred).\n",
    "      - For the adversary branch, categorical crossentropy to predict S.\n",
    "\n",
    "    Returns a compiled Keras model that takes inputs X and S (one-hot encoded) and outputs:\n",
    "      [pseudo_Y, S_pred, Y_pred].\n",
    "    \"\"\"\n",
    "    X_input = tf.keras.Input(shape=(input_dim,), name=\"X\")\n",
    "    S_input = tf.keras.Input(shape=(2,), name=\"S\")  # one-hot encoded S\n",
    "\n",
    "    # Main branch: Encoder for pseudo-label.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    h = Dense(64, activation='relu')(X_input)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Dense(32, activation='relu')(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    pseudo_Y = Dense(1, activation='sigmoid', name=\"pseudo_Y\")(h) ## outputs  probability value for pseudo_Y between 0,1\n",
    "\n",
    "    # Adversary branch: from pseudo_Y, with GRL.\n",
    "    \"\"\"\n",
    "    This is to prevent Y' from containing information about S\n",
    "    - adversary will try to predict S from pseudo_Y (fair label)...if it can accurately predict S, then Y' still encodes information about S (don't want this) \n",
    "    - use the gradient reversal layer to prevent this from happening\n",
    "    \"\"\"\n",
    "    grl = GradientReversalLayer(lambda_=lambda_adv)(pseudo_Y)\n",
    "    a = Dense(32, activation='relu')(grl)\n",
    "    a = BatchNormalization()(a)\n",
    "    S_pred = Dense(2, activation='softmax', name=\"S_pred\")(a)\n",
    "\n",
    "    # Decoder branch: combine pseudo_Y and S to predict observed Y.\n",
    "    \"\"\"\n",
    "    Y depepends on both Y' and S \n",
    "    -- predict the final observed label Y using both psuedo_Y and S\n",
    "    -- Y may still depend on S, that is why it's being used here \n",
    "    -- decoder ensures Y_final is accurate, while psuedo_Y is not directly influenced by S \n",
    "    -- psuedo_Y removes unfair dependencies on S...however S might still contain legit info needed to predict Y accurately \n",
    "    -- IMPORTANT - THIS STEP ALLOWS FAIR DEPENDENCIES WHILE ELIMINATING UNFAIR ONES\n",
    "    -- structure how S influences Y, without letting hidden biases leak through \n",
    "    \"\"\"\n",
    "    concat = Concatenate()([pseudo_Y, S_input])\n",
    "    d = Dense(16, activation='relu')(concat)\n",
    "    d = BatchNormalization()(d)\n",
    "    Y_pred = Dense(1, activation='sigmoid', name=\"Y_pred\")(d)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[X_input, S_input],\n",
    "                           outputs=[pseudo_Y, S_pred, Y_pred])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  loss={\"pseudo_Y\": \"binary_crossentropy\",\n",
    "                        \"S_pred\": \"categorical_crossentropy\",\n",
    "                        \"Y_pred\": \"binary_crossentropy\"},\n",
    "                  loss_weights={\"pseudo_Y\": 1.0, \"S_pred\": lambda_adv, \"Y_pred\": 1.0},\n",
    "                  metrics={\"pseudo_Y\": \"accuracy\",\n",
    "                           \"S_pred\": \"accuracy\",\n",
    "                           \"Y_pred\": \"accuracy\"}) # Y_pred is the best estimate of Y accounting for fair dependencies \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce495e-4c4f-4024-81ae-ec51509beaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Manual Fairness Metrics\n",
    "# -------------------------------\n",
    "def compute_fairness_metrics_manual(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Compute fairness metrics manually.\n",
    "    y_true: binary ground-truth labels (1-D numpy array).\n",
    "    y_pred: continuous scores (will be thresholded at 0.5).\n",
    "    sensitive_features: 1-D numpy array (0 or 1).\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - Demographic parity difference (absolute difference in positive rates).\n",
    "      - Equalized odds difference (average difference in TPR and FPR).\n",
    "      - Selection rates per group.\n",
    "      - Group-wise accuracy.\n",
    "    \"\"\"\n",
    "    y_pred_bin = (y_pred > 0.5).astype(int) # y_pred is continuous value, so converting it to binary \n",
    "    groups = np.unique(sensitive_features)\n",
    "\n",
    "    # Demographic parity \n",
    "    \"\"\"\n",
    "    All groups (from sensitive feature) should receive positive predictions at the same rate\n",
    "    P(Y_hat = 1|S=0) = P(Y_hat=1|S=1)\n",
    "    \"\"\"\n",
    "\n",
    "    # For each group in the sensitive feature, find the demographic parity and compute the difference (based on the formula in above comment)\n",
    "    pos_rates = {}\n",
    "    for g in groups: \n",
    "        pos_rates[g] = np.mean(y_pred_bin[sensitive_features == g])\n",
    "    dp_diff = abs(pos_rates[0] - pos_rates[1]) ## this line assumes that there are only 2 groups, 0 and 1 -- if there are more than 2 groups, this would need to be changed\n",
    "    ## in all the examples used, there were only 2 groups -- need to double check this when working on new data\n",
    "    \n",
    "    # dp_diff > 0, then demographic parity isn't fair \n",
    "\n",
    "    # Equalized odds\n",
    "    \"\"\"\n",
    "    Ensuring the different groups in the sensitive feature similar TPR and FPR rates -- this is so that the model isn't discriminating in error types\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for g in groups:\n",
    "        mask = (sensitive_features == g)\n",
    "        y_true_g = y_true[mask]\n",
    "        y_pred_g = y_pred_bin[mask]\n",
    "        tpr = np.sum((y_pred_g == 1) & (y_true_g == 1)) / (np.sum(y_true_g == 1) + 1e-8) # True Positive Rate\n",
    "        fpr = np.sum((y_pred_g == 1) & (y_true_g == 0)) / (np.sum(y_true_g == 0) + 1e-8) # False Positive Rate\n",
    "        metrics[g] = (tpr, fpr)\n",
    "    eo_diff = (abs(metrics[0][0] - metrics[1][0]) + abs(metrics[0][1] - metrics[1][1])) /  # taking average of two error types\n",
    "\n",
    "    # Selection rate per group.\n",
    "    \"\"\"\n",
    "    proportion of samples predicted as positive for each group -- a a group has a higher selection rate, the model may favor that group unfairly\n",
    "    \"\"\"\n",
    "    sel_rate = {}\n",
    "    for g in groups:\n",
    "        sel_rate[g] = pos_rates[g]\n",
    "\n",
    "    # Group-wise accuracy.\n",
    "    \"\"\"\n",
    "    for each group in the sensitive feature, compute the accuracy of the model (to ensure that it's perfoming consistently across groups)\n",
    "    \"\"\"\n",
    "    group_acc = {}\n",
    "    for g in groups:\n",
    "        mask = (sensitive_features == g)\n",
    "        group_acc[g] = accuracy_score(y_true[mask], y_pred_bin[mask])\n",
    "\n",
    "    return {\n",
    "        \"demographic_parity_difference\": dp_diff,\n",
    "        \"equalized_odds_difference\": eo_diff,\n",
    "        \"selection_rate\": sel_rate,\n",
    "        \"group_accuracy\": group_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ef3cd-b252-45a1-a259-d6b4dd09001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Plotting Function\n",
    "# -------------------------------\n",
    "def plot_comparison(metrics_baseline, metrics_fair):\n",
    "    \"\"\"\n",
    "    parameters are dictionaries with the stored values of the evaluation metrics\n",
    "    \"\"\"\n",
    "    models = ['Baseline', 'Fair']\n",
    "    aucs = [metrics_baseline['auc'], metrics_fair['auc']]\n",
    "    accs = [metrics_baseline['accuracy'], metrics_fair['accuracy']]\n",
    "    dp_diff = [metrics_baseline[\"demographic_parity_difference\"], metrics_fair[\"demographic_parity_difference\"]]\n",
    "    eo_diff = [metrics_baseline[\"equalized_odds_difference\"], metrics_fair[\"equalized_odds_difference\"]]\n",
    "\n",
    "    # creating a 2x3 gird of bar chars comparing baseline model and fair model across: AUC, accuracy, demographic parity diff, equalized odd difference\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    ## measures how well the model seperates postiive and negative classes, higher AUC = better model performance\n",
    "    # if fair model has a lower AUC than the baseline, can indicate a fairness-performance tradeoff (meaning less well seperation for more fair results)\n",
    "    axs[0,0].bar(models, aucs, color=['blue', 'green'])\n",
    "    axs[0,0].set_title('AUC')\n",
    "    axs[0,0].set_ylim([0, 1])\n",
    "\n",
    "    ## correct pred/total pred\n",
    "    ## fairness may lower accuracy \n",
    "    axs[0,1].bar(models, accs, color=['blue', 'green'])\n",
    "    axs[0,1].set_title('Accuracy')\n",
    "    axs[0,1].set_ylim([0, 1])\n",
    "\n",
    "    ## orange = baseline, purple = fairness -LOOK INTO TO SEE HOW TO KNOW WHICH GROUP IS CONTRIBUTING TO HIGHER DP\n",
    "    # lower values of dp indciate better fairness\n",
    "    axs[1,0].bar(models, dp_diff, color=['orange', 'purple'])\n",
    "    axs[1,0].set_title('Demographic Parity Difference')\n",
    "\n",
    "    ## lower value - better fairness\n",
    "    ## equalized odds is satisfied if tpr and fpr are equal across the different groups in the sensitive feature\n",
    "    axs[1,1].bar(models, eo_diff, color=['orange', 'purple'])\n",
    "    axs[1,1].set_title('Equalized Odds Difference')\n",
    "\n",
    "    plt.suptitle(\"Comparison: Baseline (X → Y) vs. Fair (X → Y') Model\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934d0d9-03c6-4480-a9d2-90283ba9e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Main Function: Comparison and Visualization\n",
    "# -------------------------------\n",
    "def main(data_url, lambda_adv=1.0):\n",
    "    set_seed(42)\n",
    "\n",
    "    print(\"Loading and preprocessing Adult data...\")\n",
    "    X, Y_obs, S = load_and_preprocess_adult_data(data_url) ##  S is binary\n",
    "    X_train, X_test, Y_train_obs, Y_test_obs, S_train, S_test = train_test_split(\n",
    "        X, Y_obs, S, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Observed Label Y shape: {Y_obs.shape}   (Label from 'income')\")\n",
    "    print(f\"Sensitive Attribute (Sex) shape: {S.shape}\")\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # One-hot encode S for adversarial model training.\n",
    "    S_train_oh = tf.keras.utils.to_categorical(S_train, num_classes=2)\n",
    "    S_test_oh  = tf.keras.utils.to_categorical(S_test, num_classes=2)\n",
    "\n",
    "    ### 1. Train adversarial debiasing model (X → Y' with adversary)\n",
    "    print(\"\\nTraining adversarial model (X → Y' with adversary) ...\")\n",
    "    adv_model = build_adversarial_model(input_dim, lambda_adv=lambda_adv)\n",
    "    # For training, we use the observed Y as target for both pseudo_Y and Y_pred.\n",
    "    # Reshape Y_obs to (-1,1) since our outputs are scalars.\n",
    "    Y_train_obs_exp = Y_train_obs.reshape(-1, 1)\n",
    "    Y_test_obs_exp  = Y_test_obs.reshape(-1, 1)\n",
    "    adv_model.fit([X_train, S_train_oh],\n",
    "                  {\"pseudo_Y\": Y_train_obs_exp, \"S_pred\": S_train_oh, \"Y_pred\": Y_train_obs_exp},\n",
    "                  epochs=30, batch_size=128, verbose=1)\n",
    "\n",
    "    # Get pseudo-label predictions.\n",
    "    pseudo_Y_train, _, _ = adv_model.predict([X_train, S_train_oh]) ## do we want psuedo_Y or Y_pred? psuedo_Y is for complete fairness why pred_Y can be a bit more accurate by keep necessary dependencies\n",
    "    pseudo_Y_test,  _, _ = adv_model.predict([X_test, S_test_oh])\n",
    "\n",
    "    # Threshold pseudo-labels to get binary labels.\n",
    "    pseudo_Y_train_bin = (pseudo_Y_train > 0.5).astype(np.float32)\n",
    "    pseudo_Y_test_bin  = (pseudo_Y_test > 0.5).astype(np.float32)\n",
    "\n",
    "    print(\"\\nPseudo-label statistics (training):\")\n",
    "    for g in np.unique(S_train):\n",
    "        mask = (S_train == g)\n",
    "        print(f\"Group {g} pseudo-positive rate: {np.mean(pseudo_Y_train_bin[mask]):.4f}\") # average probability of a postive prediction per group -- fairness check to see if both groups receive similar treatment\n",
    "\n",
    "    ### 2. Train baseline logistic regression model on observed Y (X → Y) -- regular logistic regression for baseline for comparison; does not include any fairness constraints\n",
    "    print(\"\\nTraining baseline logistic regression classifier (X → Y)...\")\n",
    "    baseline_clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    baseline_clf.fit(X_trai\n",
    "n, Y_train_obs)\n",
    "    baseline_preds = baseline_clf.predict_proba(X_test)[:, 1]\n",
    "    baseline_auc = roc_auc_score(Y_test_obs, baseline_preds)\n",
    "    baseline_acc = accuracy_score(Y_test_obs, (baseline_preds > 0.5).astype(int))\n",
    "    baseline_fairness = compute_fairness_metrics_manual(Y_test_obs, baseline_preds, sensitive_features=S_test)\n",
    "\n",
    "    ### 3. Train fair logistic regression model on pseudo-labels (X → Y') -- using psuedo_Y from the the adv_model, \n",
    "    print(\"\\nTraining fair logistic regression classifier (X → Y') using pseudo-labels...\")\n",
    "    fair_clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    fair_clf.fit(X_train, pseudo_Y_train_bin.ravel())\n",
    "    fair_preds = fair_clf.predict_proba(X_test)[:, 1]\n",
    "    fair_auc = roc_auc_score(Y_test_obs, fair_preds)\n",
    "    fair_acc = accuracy_score(Y_test_obs, (fair_preds > 0.5).astype(int))\n",
    "    fair_fairness = compute_fairness_metrics_manual(Y_test_obs, fair_preds, sensitive_features=S_test)\n",
    "\n",
    "    # Aggregate metrics for plotting.\n",
    "    metrics_baseline = {\n",
    "        \"auc\": baseline_auc,\n",
    "        \"accuracy\": baseline_acc,\n",
    "        \"demographic_parity_difference\": baseline_fairness[\"demographic_parity_difference\"],\n",
    "        \"equalized_odds_difference\": baseline_fairness[\"equalized_odds_difference\"]\n",
    "    }\n",
    "    metrics_fair = {\n",
    "        \"auc\": fair_auc,\n",
    "        \"accuracy\": fair_acc,\n",
    "        \"demographic_parity_difference\": fair_fairness[\"demographic_parity_difference\"],\n",
    "        \"equalized_odds_difference\": fair_fairness[\"equalized_odds_difference\"]\n",
    "    }\n",
    "\n",
    "    print(\"\\nBaseline Logistic Regression (X → Y) Evaluation:\")\n",
    "    print(f\"AUC: {baseline_auc:.4f}, Accuracy: {baseline_acc:.4f}\")\n",
    "    print(\"Fairness metrics:\", baseline_fairness)\n",
    "\n",
    "    print(\"\\nFair Logistic Regression (X → Y') Evaluation (compared to observed Y):\")\n",
    "    print(f\"AUC: {fair_auc:.4f}, Accuracy: {fair_acc:.4f}\")\n",
    "    print(\"Fairness metrics:\", fair_fairness)\n",
    "\n",
    "    # Plot comparison.\n",
    "    plot_comparison(metrics_baseline, metrics_fair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a7324-619d-470b-9ec7-f1b4bf6d984f",
   "metadata": {},
   "source": [
    "### Application on Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73494a4-59ca-4284-a545-09d160dadc83",
   "metadata": {},
   "source": [
    "#### UCI Adults Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1178730-cf43-4c84-b4d6-958437dde3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_adult_data(data_url):\n",
    "    \"\"\"\n",
    "    Download and preprocess the UCI Adult dataset.\n",
    "\n",
    "    Features (X): use only:\n",
    "       'age', 'education-num', 'marital-status', 'occupation', 'hours-per-week'\n",
    "    Observed Label (Y): derived from 'income' (binary: 1 if '>50K', 0 otherwise)\n",
    "    Sensitive attribute (S): derived from 'sex' (binary: 1 if 'Male', 0 if 'Female')\n",
    "\n",
    "    Returns:\n",
    "      X: numpy array of shape (n_samples, 5)\n",
    "      Y: 1-D numpy array of observed labels.\n",
    "      S: 1-D numpy array of sensitive attribute.\n",
    "    \"\"\"\n",
    "    col_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "                 \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "                 \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
    "    data = pd.read_csv(data_url, header=None, names=col_names, na_values=\" ?\", skipinitialspace=True)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Features\n",
    "    feature_cols = [\"age\", \"education-num\", \"marital-status\", \"occupation\", \"hours-per-week\"]\n",
    "    X = data[feature_cols].copy()\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == object:\n",
    "            X[col] = pd.Categorical(X[col]).codes\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X.values.astype(np.float32))\n",
    "\n",
    "    # Observed label\n",
    "    data['income'] = data['income'].apply(lambda s: s.replace('.', '').strip())\n",
    "    y_binary = (data['income'] == '>50K').astype(np.int32)\n",
    "    Y = y_binary.values  # 1-D array\n",
    "\n",
    "    # Sensitive attribute\n",
    "    S = (data['sex'].str.strip() == 'Male').astype(np.int32).values  # 1-D array\n",
    "\n",
    "    return X, Y, S\n",
    "\n",
    "\n",
    "adult_data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "main(adult_data_url, lambda_adv=5.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300fe20-2a86-47cd-94c9-733ebf3d57de",
   "metadata": {},
   "source": [
    "#### German Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937fee6-e7fe-4bb7-a4c1-995b0b9c76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_german_data(data_url):\n",
    "    \"\"\"\n",
    "    Download and preprocess the German Credit dataset.\n",
    "\n",
    "    We assume the dataset has 21 columns.\n",
    "\n",
    "    Features (X): Use only:\n",
    "        \"duration\", \"credit_amount\", \"inst_rate\"\n",
    "    Observed Label (Y): from \"target\". In many versions, target is coded as 1 for good and 2 for bad.\n",
    "        We recode: good (1) -> 1, bad (2) -> 0.\n",
    "    Protected Attribute (S): Use the \"age\" column.\n",
    "        We binarize age by computing the median and setting S = 1 if age >= median (older), else 0.\n",
    "    \"\"\"\n",
    "    col_names = [\"chk_status\", \"duration\", \"credit_history\", \"purpose\", \"credit_amount\",\n",
    "                 \"savings\", \"employment\", \"inst_rate\", \"personal_status_sex\", \"other_debtors\",\n",
    "                 \"residence_since\", \"property\", \"age\", \"other_installment_plans\", \"housing\",\n",
    "                 \"num_credits\", \"job\", \"num_dependents\", \"telephone\", \"foreign_worker\", \"target\"]\n",
    "    data = pd.read_csv(data_url, header=None, names=col_names, sep=' ', engine='python')\n",
    "\n",
    "    # Features: use only duration, credit_amount, and inst_rate.\n",
    "    feature_cols = [\"duration\", \"credit_amount\", \"inst_rate\"]\n",
    "    X = data[feature_cols].copy().astype(np.float32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X.values)\n",
    "\n",
    "    # Observed label: target. Recode so that 1 -> 1 (good) and 2 -> 0 (bad)\n",
    "    Y = data[\"target\"].values\n",
    "    Y = np.where(Y == 1, 1, 0)\n",
    "\n",
    "    # Protected attribute: use age.\n",
    "    # Convert age to float and then binarize by the median.\n",
    "    age_vals = data[\"age\"].astype(np.float32).values\n",
    "    median_age = np.median(age_vals)\n",
    "    S = (age_vals >= median_age).astype(np.int32)\n",
    "\n",
    "    return X, Y, S\n",
    "\n",
    "\n",
    "\n",
    "german_data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\n",
    "main(german_data_url, lambda_adv=15.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e932ef6-8118-4e10-b27a-c3eb82646ffe",
   "metadata": {},
   "source": [
    "#### Compas Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c2a51-e487-4b97-bb68-00c4d6d0b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_compas_data(data_url):\n",
    "    \"\"\"\n",
    "    Download and preprocess the COMPAS dataset.\n",
    "\n",
    "    We assume the dataset contains, among others, the following columns:\n",
    "      - 'age'\n",
    "      - 'race'\n",
    "      - 'priors_count'\n",
    "      - 'juv_fel_count'\n",
    "      - 'juv_misd_count'\n",
    "      - 'juv_other_count'\n",
    "      - 'two_year_recid'\n",
    "\n",
    "    Features (X): We select a few numerical features.\n",
    "    Observed Label (Y): Use 'two_year_recid' as a binary label (0/1).\n",
    "    Protected Attribute (S): Use 'race'. Here we binarize race so that:\n",
    "         African‑American  → 1\n",
    "         all other races  → 0.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_url)\n",
    "    # Drop rows with missing values in the selected columns.\n",
    "    data = data.dropna(subset=[\"age\", \"race\", \"priors_count\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\", \"two_year_recid\"])\n",
    "\n",
    "    # Observed label: two_year_recid (already 0/1)\n",
    "    Y = data[\"two_year_recid\"].values\n",
    "\n",
    "    # Sensitive attribute: race. We set S=1 if race is African-American, else 0.\n",
    "    S = (data[\"race\"] == \"African-American\").astype(int).values\n",
    "\n",
    "    # Features: use a subset of numerical features.\n",
    "    feature_cols = [\"age\", \"priors_count\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\"]\n",
    "    X = data[feature_cols].copy().astype(np.float32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X.values)\n",
    "\n",
    "    return X, Y, S\n",
    "\n",
    "# URL for the ProPublica COMPAS dataset\n",
    "compas_data_url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "# You can adjust lambda_adv as desired (e.g., lambda_adv=15.5 as in your German data experiment)\n",
    "main_compas(compas_data_url, lambda_adv=3.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
