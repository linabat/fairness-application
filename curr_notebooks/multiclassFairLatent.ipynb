{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24672b93-3c9d-4e08-98b2-b6a106a323bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 12:52:22.198639: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-10 12:52:22.198683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-10 12:52:22.200204: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-10 12:52:22.210321: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Flatten, \n",
    "    MaxPooling2D, BatchNormalization, Dropout\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Flatten, \n",
    "    MaxPooling2D, BatchNormalization, Dropout, Concatenate\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler\n",
    ")\n",
    "\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9af067-aa00-404e-9bbb-9fa55fd4367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Custom Gradient Reversal Layer\n",
    "# -------------------------------\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x, lambda_):\n",
    "    def grad(dy):\n",
    "        return -lambda_ * dy, None # reverses direction of gradient \n",
    "    return x, grad\n",
    "\n",
    "# custom Keras layer\n",
    "\"\"\"\n",
    "Layer is used to ensure that the feature representation are independent of a sensitive attribute\n",
    "- feature extract learns normally in the forward pass\n",
    "- reversing gradients of classifier that tries to predict the sensitive attribute during backpropagation -- stops feature extractor from encoding sensitive information\n",
    "\"\"\"\n",
    "class GradientReversalLayer(tf.keras.layers.Layer): \n",
    "    def __init__(self, lambda_=1.0, **kwargs):\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "        self.lambda_ = lambda_ # strength of gradient reversal\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x, self.lambda_)\n",
    "\n",
    "# -------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# -------------------------------\n",
    "def set_seed(seed_num):\n",
    "    random.seed(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a279a6-a67d-4bab-8049-18a4eb1ab4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Adversarial Debiasing Model\n",
    "# -------------------------------\n",
    "\n",
    "## has been adjusted for multiclass\n",
    "def build_adversarial_model(input_dim, num_classes_Y, lambda_adv=1.0):\n",
    "    \"\"\"\n",
    "    Build an adversarial debiasing model that learns pseudo‑labels Y' from X.\n",
    "\n",
    "    Architecture:\n",
    "      - Main branch (encoder): from X, several dense layers produce a latent pseudo‑label pseudo_Y (via sigmoid).\n",
    "      - Adversary branch: pseudo_Y is passed through a Gradient Reversal Layer and then dense layers predict S.\n",
    "      - Decoder branch: concatenates pseudo_Y and the one-hot sensitive attribute S to predict the observed label Y.\n",
    "\n",
    "    Losses:\n",
    "      - For the main branch, binary crossentropy between observed Y and pseudo_Y (and Y_pred).\n",
    "      - For the adversary branch, categorical crossentropy to predict S.\n",
    "\n",
    "    Returns a compiled Keras model that takes inputs X and S (one-hot encoded) and outputs:\n",
    "      [pseudo_Y, S_pred, Y_pred].\n",
    "    \"\"\"\n",
    "    X_input = tf.keras.Input(shape=(input_dim,), name=\"X\")\n",
    "    S_input = tf.keras.Input(shape=(2,), name=\"S\")  # one-hot encoded S\n",
    "\n",
    "    # Main branch: Encoder for pseudo-label.\n",
    "    h = Dense(64, activation='relu')(X_input)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Dense(32, activation='relu')(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    pseudo_Y = Dense(num_classes_Y, activation='softmax', name=\"pseudo_Y\")(h) ## changed to softmax because multi-class\n",
    "\n",
    "    # Adversary branch: from pseudo_Y, with GRL.\n",
    "    \"\"\"\n",
    "    This is to prevent psuedo_Y from containing information about S\n",
    "    - adversary will try to predict S from pseudo_Y (fair label)...if it can accurately predict S, then Y' still encodes information about S (don't want this) \n",
    "    - use the gradient reversal layer to prevent this from happening\n",
    "    \"\"\"\n",
    "    grl = GradientReversalLayer(lambda_=lambda_adv)(pseudo_Y)\n",
    "    a = Dense(32, activation='relu')(grl)\n",
    "    a = BatchNormalization()(a)\n",
    "    S_pred = Dense(2, activation='softmax', name=\"S_pred\")(a)\n",
    "\n",
    "    # Decoder branch: combine pseudo_Y and S to predict observed Y.\n",
    "    concat = Concatenate()([pseudo_Y, S_input])\n",
    "    d = Dense(16, activation='relu')(concat)\n",
    "    d = BatchNormalization()(d)\n",
    "    Y_pred = Dense(num_classes_Y, activation='softmax', name=\"Y_pred\")(d) # changed from 1 to num_classes_Y, changed to softmax cause multi\n",
    "\n",
    "    model = tf.keras.Model(inputs=[X_input, S_input],\n",
    "                           outputs=[pseudo_Y, S_pred, Y_pred])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  loss={\"pseudo_Y\": \"categorical_crossentropy\", # changed from binary to categorical\n",
    "                        \"S_pred\": \"categorical_crossentropy\",\n",
    "                        \"Y_pred\": \"categorical_crossentropy\"}, # changed from binary to categorical\n",
    "                  loss_weights={\"pseudo_Y\": 1.0, \"S_pred\": lambda_adv, \"Y_pred\": 1.0},\n",
    "                  metrics={\"pseudo_Y\": \"accuracy\",\n",
    "                           \"S_pred\": \"accuracy\",\n",
    "                           \"Y_pred\": \"accuracy\"}) # Y_pred is the best estimate of Y accounting for fair dependencies \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdce495e-4c4f-4024-81ae-ec51509beaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Manual Fairness Metrics\n",
    "# -------------------------------\n",
    "\n",
    "# changed for multi\n",
    "def multi_compute_fairness_metrics_manual(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Compute fairness metrics manually.\n",
    "    y_true: binary ground-truth labels (1-D numpy array).\n",
    "    y_pred: continuous scores (will be thresholded at 0.5).\n",
    "    sensitive_features: 1-D numpy array (0 or 1).\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - Demographic parity difference (absolute difference in positive rates).\n",
    "      - Equalized odds difference (average difference in TPR and FPR).\n",
    "      - Selection rates per group.\n",
    "      - Group-wise accuracy.\n",
    "    \"\"\"\n",
    "    y_pred_bin = np.argmax(y_pred, axis=1) # converting probability to class prediction\n",
    "    groups = np.unique(sensitive_features) # all the different groups in the sensitive feature)\n",
    "    classes = np.unique(y_true) # all the different classes in y_true (for multiclass)\n",
    "\n",
    "    # Demographic parity \n",
    "\n",
    "    # For each group in the sensitive feature, find the demographic parity and compute the difference (based on the formula in above comment) -- will look at each proportion per class\n",
    "    class_rates = {g: np.zeros(len(unique_classes)) for g in unique_groups}\n",
    "\n",
    "    for g in groups:\n",
    "        mask = (sensitive_features == g) \n",
    "        for cl in classes: \n",
    "            class_rates[g][c] = np.mean(y_pred_class[mask] == c)\n",
    "    dp_diff = np.max([np.abs(class_rates[g1] - class_rates[g2]) for g1 in groups for g2 in groups if g1 != g2])\n",
    "\n",
    "\n",
    "    # Equalized odds\n",
    "    \"\"\"\n",
    "    Ensuring the different groups in the sensitive feature similar TPR and FPR rates -- this is so that the model isn't discriminating in error types\n",
    "    \"\"\"\n",
    "    metrics = {g: {c: {\"TPR\": 0, \"FPR\": 0} for c in classes} for g in groups}\n",
    "\n",
    "    for g in groups:\n",
    "        mask = (sensitive_features == g)\n",
    "        y_true_g = y_true[mask]\n",
    "        y_pred_g = y_pred_bin[mask]\n",
    "\n",
    "        for c in unique_classes:\n",
    "            tp = np.sum((y_pred_g == c) & (y_true_g == c))\n",
    "            fn = np.sum((y_pred_g != c) & (y_true_g == c))\n",
    "            fp = np.sum((y_pred_g == c) & (y_true_g != c))\n",
    "            tn = np.sum((y_pred_g != c) & (y_true_g != c))\n",
    "\n",
    "            metrics[g][c][\"TPR\"] = tp / (tp + fn) # true positive\n",
    "            metrics[g][c][\"FPR\"] = fp / (fp + tn) # false positive\n",
    "\n",
    "        \n",
    "    eo_diff_vals = []\n",
    "    for g1 in groups:\n",
    "        for g2 in groups: \n",
    "            if g1 != g2: # trying to compare tpr and fpr across the different groups\n",
    "                for c in classes: \n",
    "                    tpr_diff = np.abs(metrics[g1][c][\"TPR\"] - metrics[g2][c][\"TPR\"])\n",
    "                    fpr_diff = np.abs(metrics[g1][c][\"FPR\"] - metrics[g2][c][\"FPR\"])\n",
    "                    eo_diff_vals.append(tpr_diff + fpr_diff)\n",
    "    eof_diff = np.max(eo_diff_vals)\n",
    "                \n",
    "    \n",
    "    # Selection rate per group.\n",
    "    \"\"\"\n",
    "    proportion of samples predicted as positive for each group -- a group has a higher selection rate, the model may favor that group unfairly\n",
    "    \"\"\"\n",
    "    selection_rate = {g: class_rates[g].tolist() for g in groups}\n",
    "\n",
    "    # Group-wise accuracy.\n",
    "    \"\"\"\n",
    "    for each group in the sensitive feature, compute the accuracy of the model (to ensure that it's perfoming consistently across groups)\n",
    "    \"\"\"\n",
    "    group_acc = {}\n",
    "    for g in groups:\n",
    "        mask = (sensitive_features == g)\n",
    "        group_acc[g] = accuracy_score(y_true[mask], y_pred_bin[mask])\n",
    "\n",
    "    return {\n",
    "        \"demographic_parity_difference\": dp_diff,\n",
    "        \"equalized_odds_difference\": eo_diff,\n",
    "        \"selection_rate\": sel_rate,\n",
    "        \"group_accuracy\": group_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215ef3cd-b252-45a1-a259-d6b4dd09001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Plotting Function\n",
    "# -------------------------------\n",
    "\n",
    "# changed for multi class\n",
    "def multi_plot_comparison(metrics_baseline, metrics_fair):\n",
    "    \"\"\"\n",
    "    parameters are dictionaries with the stored values of the evaluation metrics\n",
    "    \"\"\"\n",
    "    models = ['Baseline', 'Fair']\n",
    "    aucs = [metrics_baseline['auc'], metrics_fair['auc']]\n",
    "    accs = [metrics_baseline['accuracy'], metrics_fair['accuracy']]\n",
    "    dp_diff = [metrics_baseline[\"demographic_parity_difference\"], metrics_fair[\"demographic_parity_difference\"]]\n",
    "    eo_diff = [metrics_baseline[\"equalized_odds_difference\"], metrics_fair[\"equalized_odds_difference\"]]\n",
    "\n",
    "    # creating a 2x3 gird of bar chars comparing baseline model and fair model across: AUC, accuracy, demographic parity diff, equalized odd difference\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    ## measures how well the model seperates postiive and negative classes, higher AUC = better model performance\n",
    "    # if fair model has a lower AUC than the baseline, can indicate a fairness-performance tradeoff (meaning less well seperation for more fair results)\n",
    "    axs[0,0].bar(models, aucs, color=['blue', 'green'])\n",
    "    axs[0,0].set_title('AUC')\n",
    "    axs[0,0].set_ylim([0, 1])\n",
    "\n",
    "    ## correct pred/total pred\n",
    "    ## fairness may lower accuracy \n",
    "    axs[0,1].bar(models, accs, color=['blue', 'green'])\n",
    "    axs[0,1].set_title('Accuracy')\n",
    "    axs[0,1].set_ylim([0, 1])\n",
    "\n",
    "    ## orange = baseline, purple = fairness -LOOK INTO TO SEE HOW TO KNOW WHICH GROUP IS CONTRIBUTING TO HIGHER DP\n",
    "    # lower values of dp indciate better fairness\n",
    "    axs[1,0].bar(models, dp_diff, color=['orange', 'purple'])\n",
    "    axs[1,0].set_title('Demographic Parity Difference')\n",
    "\n",
    "    ## lower value - better fairness\n",
    "    ## equalized odds is satisfied if tpr and fpr are equal across the different groups in the sensitive feature\n",
    "    axs[1,1].bar(models, eo_diff, color=['orange', 'purple'])\n",
    "    axs[1,1].set_title('Equalized Odds Difference')\n",
    "\n",
    "    plt.suptitle(\"Comparison: Baseline (X → Y) vs. Fair (X → Y') Model\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9934d0d9-03c6-4480-a9d2-90283ba9e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Main Function: Comparison and Visualization\n",
    "# -------------------------------\n",
    "def multi_main(data_url, dataset_name, lambda_adv=1.0):\n",
    "    set_seed(42)\n",
    "\n",
    "    if dataset_name == \"compas\": \n",
    "        X, Y_obs, S = load_and_preprocess_compas_data_binary(data_url) ##  Y, S is binary\n",
    "        num_classes_Y = len(np.unique(Y_obs))\n",
    "    \n",
    "    elif dataset_name == \"drug\":\n",
    "        X, Y_obs, S = load_and_process_drug_consumption_data(data_url) ##  Y is multi class, S is binary\n",
    "        num_classes_Y = len(np.unique(Y_obs))\n",
    "\n",
    "    else:\n",
    "        print (\"Invalid dataset_name\")\n",
    "        return \n",
    "    \n",
    "    print(f\"Loading and preprocessing {dataset_name} data...\")\n",
    "    X_train, X_test, Y_train_obs, Y_test_obs, S_train, S_test = train_test_split(\n",
    "        X, Y_obs, S, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    if dataset_name == \"compas\":\n",
    "        print(f\"Features shape: {X.shape}\")\n",
    "        print(f\"Observed Label Y shape: {Y_obs.shape}   (Recidivism: 1=recid, 0=non-recid)\")\n",
    "        print(f\"Sensitive Attribute (Race, binarized) shape: {S.shape}\")\n",
    "        \n",
    "    elif dataset_name == \"drug\":\n",
    "        print(f\"Features shape: {X.shape}\")\n",
    "        print(f\"Observed Label Y shape: {Y_obs.shape}   (Label from 'drug consumption')\")\n",
    "        print(f\"Sensitive Attribute (Education) shape: {S.shape}\")\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # One-hot encode S for adversarial model training.\n",
    "    S_train_oh = tf.keras.utils.to_categorical(S_train, num_classes=2) # will need to change this if not longer 2\n",
    "    S_test_oh  = tf.keras.utils.to_categorical(S_test, num_classes=2) # will need to change this if no longer 2\n",
    "\n",
    "    # need to one hot encode Y\n",
    "    Y_train_obs = tf.keras.utils.to_categorical(Y_train_obs, num_classes=num_classes_Y)\n",
    "    Y_test_obs = tf.keras.utils.to_categorical(Y_test_obs, num_classes=num_classes_Y)\n",
    "    \n",
    "    Y_train_obs_1d = np.argmax(Y_train_obs, axis=1)  # Convert from one-hot to categorical labels\n",
    "    Y_test_obs_1d = np.argmax(Y_test_obs, axis=1)  # Do the same for test set\n",
    "\n",
    "    ### 1. Train adversarial debiasing model (X → Y' with adversary)\n",
    "    print(\"\\nTraining adversarial model (X → Y' with adversary) ...\")\n",
    "    adv_model = build_adversarial_model(input_dim, num_classes_Y, lambda_adv=lambda_adv)\n",
    "    # For training, we use the observed Y as target for both pseudo_Y and Y_pred.\n",
    "    # Reshape Y_obs to (-1,1) since our outputs are scalars.\n",
    "    # Y_train_obs_exp = Y_train_obs.reshape(-1, 1)\n",
    "    # Y_test_obs_exp  = Y_test_obs.reshape(-1, 1)\n",
    "    adv_model.fit([X_train, S_train_oh],\n",
    "                  {\"pseudo_Y\": Y_train_obs, \"S_pred\": S_train_oh, \"Y_pred\": Y_train_obs},\n",
    "                  epochs=30, batch_size=128, verbose=1)\n",
    "\n",
    "    # Get pseudo-label predictions.\n",
    "    pseudo_Y_train, S_pred_train, Y_pred_train = adv_model.predict([X_train, S_train_oh]) ## changed so we're using Y_pred here instead of psuedo_Y\n",
    "    pseudo_Y_test,  S_pred_test, Y_pred_test = adv_model.predict([X_test, S_test_oh])\n",
    "\n",
    "    # Threshold pseudo-labels to get binary labels.\n",
    "    Y_pred_train_bin = np.argmax(Y_pred_train, axis= 1)\n",
    "    Y_pred_test_bin  = np.argmax(Y_pred_test, axis=1) \n",
    "\n",
    "    print(\"\\nPseudo-label statistics (training):\")\n",
    "    for g in np.unique(S_train):\n",
    "        mask = (S_train == g)\n",
    "        print(f\"Group {g} pseudo-positive rate: {np.mean(Y_pred_train_bin[mask]):.4f}\") # average probability of a postive prediction per group -- fairness check to see if both groups receive similar treatment\n",
    "\n",
    "    ### 2. Train baseline logistic regression model on observed Y (X → Y) -- regular logistic regression for baseline for comparison; does not include any fairness constraints\n",
    "    print(\"\\nTraining baseline logistic regression classifier (X → Y)...\")\n",
    "    baseline_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "    baseline_clf.fit(X_train, Y_train_obs_1d)\n",
    "    \n",
    "    baseline_preds = baseline_clf.predict_proba(X_test)\n",
    "    \n",
    "    baseline_auc = roc_auc_score(Y_test_obs, baseline_preds, multi_class=\"ovr\")\n",
    "    baseline_preds_class = baseline_preds.argmax(axis=1)\n",
    "    baseline_fairness = multi_compute_fairness_metrics_manual(Y_test_obs, baseline_preds_class, sensitive_features=S_test)\n",
    "\n",
    "    ### 3. Train fair logistic regression model on pseudo-labels (X → Y') -- using psuedo_Y from the the adv_model, \n",
    "    print(\"\\nTraining fair logistic regression classifier (X → Y') using Y_pred labels...\")\n",
    "    fair_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "    fair_clf.fit(X_train, Y_pred_train_bin)\n",
    "    fair_preds = fair_clf.predict_proba(X_test)\n",
    "    fair_auc = roc_auc_score(Y_test_obs, fair_preds, multi_class='ovr')\n",
    "    fair_preds_class = fair_preds.argmax(axis=1)\n",
    "    fair_fairness = multi_compute_fairness_metrics_manual(Y_test_obs, fair_preds_class, sensitive_features=S_test)\n",
    "\n",
    "    # Aggregate metrics for plotting.\n",
    "    metrics_baseline = {\n",
    "        \"auc\": baseline_auc,\n",
    "        \"accuracy\": baseline_acc,\n",
    "        \"demographic_parity_difference\": baseline_fairness[\"demographic_parity_difference\"],\n",
    "        \"equalized_odds_difference\": baseline_fairness[\"equalized_odds_difference\"]\n",
    "    }\n",
    "    metrics_fair = {\n",
    "        \"auc\": fair_auc,\n",
    "        \"accuracy\": fair_acc,\n",
    "        \"demographic_parity_difference\": fair_fairness[\"demographic_parity_difference\"],\n",
    "        \"equalized_odds_difference\": fair_fairness[\"equalized_odds_difference\"]\n",
    "    }\n",
    "\n",
    "    print(\"\\nBaseline Logistic Regression (X → Y) Evaluation:\")\n",
    "    print(f\"AUC: {baseline_auc:.4f}, Accuracy: {baseline_acc:.4f}\")\n",
    "    print(\"Fairness metrics:\", baseline_fairness)\n",
    "\n",
    "    print(\"\\nFair Logistic Regression (X → Y') Evaluation (compared to observed Y):\")\n",
    "    print(f\"AUC: {fair_auc:.4f}, Accuracy: {fair_acc:.4f}\")\n",
    "    print(\"Fairness metrics:\", fair_fairness)\n",
    "\n",
    "    # Plot comparison.\n",
    "    multi_plot_comparison(metrics_baseline, metrics_fair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a7324-619d-470b-9ec7-f1b4bf6d984f",
   "metadata": {},
   "source": [
    "### Application on Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a8cb1-26fd-47ae-ba82-01642fb05249",
   "metadata": {},
   "source": [
    "#### Drug Consumption Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49a030df-362c-4806-8fdd-75f9b4a75487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing drug data...\n",
      "Features shape: (1884, 10)\n",
      "Observed Label Y shape: (1884,)   (Label from 'drug consumption')\n",
      "Sensitive Attribute (Education) shape: (1884,)\n",
      "\n",
      "Training adversarial model (X → Y' with adversary) ...\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 2s 6ms/step - loss: 4.4865 - pseudo_Y_loss: 1.8722 - S_pred_loss: 0.9551 - Y_pred_loss: 1.6592 - pseudo_Y_accuracy: 0.2528 - S_pred_accuracy: 0.5222 - Y_pred_accuracy: 0.3019\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.3971 - pseudo_Y_loss: 1.8406 - S_pred_loss: 0.9332 - Y_pred_loss: 1.6233 - pseudo_Y_accuracy: 0.2681 - S_pred_accuracy: 0.5129 - Y_pred_accuracy: 0.2873\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 4.3746 - pseudo_Y_loss: 1.8257 - S_pred_loss: 0.9389 - Y_pred_loss: 1.6099 - pseudo_Y_accuracy: 0.2687 - S_pred_accuracy: 0.5109 - Y_pred_accuracy: 0.2960\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.3336 - pseudo_Y_loss: 1.7906 - S_pred_loss: 0.9469 - Y_pred_loss: 1.5961 - pseudo_Y_accuracy: 0.2794 - S_pred_accuracy: 0.5116 - Y_pred_accuracy: 0.3139\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.2748 - pseudo_Y_loss: 1.7762 - S_pred_loss: 0.9281 - Y_pred_loss: 1.5705 - pseudo_Y_accuracy: 0.2827 - S_pred_accuracy: 0.5083 - Y_pred_accuracy: 0.3066\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.2323 - pseudo_Y_loss: 1.7536 - S_pred_loss: 0.9275 - Y_pred_loss: 1.5512 - pseudo_Y_accuracy: 0.2860 - S_pred_accuracy: 0.5070 - Y_pred_accuracy: 0.3112\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4.1783 - pseudo_Y_loss: 1.7323 - S_pred_loss: 0.9084 - Y_pred_loss: 1.5376 - pseudo_Y_accuracy: 0.2887 - S_pred_accuracy: 0.5070 - Y_pred_accuracy: 0.3218\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4.1206 - pseudo_Y_loss: 1.7066 - S_pred_loss: 0.8925 - Y_pred_loss: 1.5215 - pseudo_Y_accuracy: 0.2986 - S_pred_accuracy: 0.5169 - Y_pred_accuracy: 0.3185\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4.1073 - pseudo_Y_loss: 1.6997 - S_pred_loss: 0.8884 - Y_pred_loss: 1.5192 - pseudo_Y_accuracy: 0.2887 - S_pred_accuracy: 0.5116 - Y_pred_accuracy: 0.3179\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 4.0394 - pseudo_Y_loss: 1.6785 - S_pred_loss: 0.8657 - Y_pred_loss: 1.4953 - pseudo_Y_accuracy: 0.3019 - S_pred_accuracy: 0.5096 - Y_pred_accuracy: 0.3378\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.0079 - pseudo_Y_loss: 1.6693 - S_pred_loss: 0.8604 - Y_pred_loss: 1.4783 - pseudo_Y_accuracy: 0.2933 - S_pred_accuracy: 0.5096 - Y_pred_accuracy: 0.3451\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.9577 - pseudo_Y_loss: 1.6462 - S_pred_loss: 0.8426 - Y_pred_loss: 1.4688 - pseudo_Y_accuracy: 0.3019 - S_pred_accuracy: 0.5070 - Y_pred_accuracy: 0.3464\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9089 - pseudo_Y_loss: 1.6328 - S_pred_loss: 0.8262 - Y_pred_loss: 1.4498 - pseudo_Y_accuracy: 0.3119 - S_pred_accuracy: 0.5036 - Y_pred_accuracy: 0.3577\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.8390 - pseudo_Y_loss: 1.6142 - S_pred_loss: 0.7965 - Y_pred_loss: 1.4284 - pseudo_Y_accuracy: 0.3026 - S_pred_accuracy: 0.5136 - Y_pred_accuracy: 0.3577\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.8060 - pseudo_Y_loss: 1.5947 - S_pred_loss: 0.7879 - Y_pred_loss: 1.4234 - pseudo_Y_accuracy: 0.3106 - S_pred_accuracy: 0.5116 - Y_pred_accuracy: 0.3656\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.7984 - pseudo_Y_loss: 1.6029 - S_pred_loss: 0.7787 - Y_pred_loss: 1.4168 - pseudo_Y_accuracy: 0.3092 - S_pred_accuracy: 0.5056 - Y_pred_accuracy: 0.3703\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.7308 - pseudo_Y_loss: 1.5680 - S_pred_loss: 0.7634 - Y_pred_loss: 1.3994 - pseudo_Y_accuracy: 0.3192 - S_pred_accuracy: 0.5063 - Y_pred_accuracy: 0.3908\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.7064 - pseudo_Y_loss: 1.5651 - S_pred_loss: 0.7502 - Y_pred_loss: 1.3911 - pseudo_Y_accuracy: 0.3238 - S_pred_accuracy: 0.5342 - Y_pred_accuracy: 0.3802\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.6651 - pseudo_Y_loss: 1.5440 - S_pred_loss: 0.7414 - Y_pred_loss: 1.3798 - pseudo_Y_accuracy: 0.3165 - S_pred_accuracy: 0.5388 - Y_pred_accuracy: 0.3908\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.6389 - pseudo_Y_loss: 1.5350 - S_pred_loss: 0.7339 - Y_pred_loss: 1.3699 - pseudo_Y_accuracy: 0.3232 - S_pred_accuracy: 0.5368 - Y_pred_accuracy: 0.3981\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.6102 - pseudo_Y_loss: 1.5207 - S_pred_loss: 0.7306 - Y_pred_loss: 1.3590 - pseudo_Y_accuracy: 0.3205 - S_pred_accuracy: 0.5328 - Y_pred_accuracy: 0.4094\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.5925 - pseudo_Y_loss: 1.5134 - S_pred_loss: 0.7193 - Y_pred_loss: 1.3598 - pseudo_Y_accuracy: 0.3338 - S_pred_accuracy: 0.5528 - Y_pred_accuracy: 0.4147\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.5557 - pseudo_Y_loss: 1.4961 - S_pred_loss: 0.7070 - Y_pred_loss: 1.3526 - pseudo_Y_accuracy: 0.3338 - S_pred_accuracy: 0.5654 - Y_pred_accuracy: 0.4180\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.5454 - pseudo_Y_loss: 1.4965 - S_pred_loss: 0.7115 - Y_pred_loss: 1.3374 - pseudo_Y_accuracy: 0.3251 - S_pred_accuracy: 0.5727 - Y_pred_accuracy: 0.4300\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.5189 - pseudo_Y_loss: 1.4916 - S_pred_loss: 0.6946 - Y_pred_loss: 1.3327 - pseudo_Y_accuracy: 0.3351 - S_pred_accuracy: 0.5912 - Y_pred_accuracy: 0.4353\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.4944 - pseudo_Y_loss: 1.4713 - S_pred_loss: 0.6968 - Y_pred_loss: 1.3262 - pseudo_Y_accuracy: 0.3451 - S_pred_accuracy: 0.5926 - Y_pred_accuracy: 0.4267\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.4872 - pseudo_Y_loss: 1.4636 - S_pred_loss: 0.6999 - Y_pred_loss: 1.3237 - pseudo_Y_accuracy: 0.3524 - S_pred_accuracy: 0.5879 - Y_pred_accuracy: 0.4207\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.4563 - pseudo_Y_loss: 1.4460 - S_pred_loss: 0.6937 - Y_pred_loss: 1.3166 - pseudo_Y_accuracy: 0.3537 - S_pred_accuracy: 0.5985 - Y_pred_accuracy: 0.4320\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 3.4465 - pseudo_Y_loss: 1.4383 - S_pred_loss: 0.6907 - Y_pred_loss: 1.3174 - pseudo_Y_accuracy: 0.3570 - S_pred_accuracy: 0.5939 - Y_pred_accuracy: 0.4340\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.4181 - pseudo_Y_loss: 1.4320 - S_pred_loss: 0.6878 - Y_pred_loss: 1.2983 - pseudo_Y_accuracy: 0.3470 - S_pred_accuracy: 0.5859 - Y_pred_accuracy: 0.4426\n",
      "48/48 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "\n",
      "Pseudo-label statistics (training):\n",
      "Group 0 pseudo-positive rate: 1.8265\n",
      "Group 1 pseudo-positive rate: 1.0458\n",
      "\n",
      "Training baseline logistic regression classifier (X → Y)...\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, Y, S\n\u001b[0;32m---> 53\u001b[0m \u001b[43mmulti_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANYTHING\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_adv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 80\u001b[0m, in \u001b[0;36mmulti_main\u001b[0;34m(data_url, dataset_name, lambda_adv)\u001b[0m\n\u001b[1;32m     78\u001b[0m baseline_auc \u001b[38;5;241m=\u001b[39m roc_auc_score(Y_test_obs, baseline_preds, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m baseline_preds_class \u001b[38;5;241m=\u001b[39m baseline_preds\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m baseline_fairness \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_compute_fairness_metrics_manual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_preds_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m### 3. Train fair logistic regression model on pseudo-labels (X → Y') -- using psuedo_Y from the the adv_model, \u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining fair logistic regression classifier (X → Y\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) using Y_pred labels...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mmulti_compute_fairness_metrics_manual\u001b[0;34m(y_true, y_pred, sensitive_features)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmulti_compute_fairness_metrics_manual\u001b[39m(y_true, y_pred, sensitive_features):\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Compute fairness metrics manually.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    y_true: binary ground-truth labels (1-D numpy array).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m      - Group-wise accuracy.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     y_pred_bin \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# converting probability to class prediction\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     groups \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(sensitive_features) \u001b[38;5;66;03m# all the different groups in the sensitive feature)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true) \u001b[38;5;66;03m# all the different classes in y_true (for multiclass)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "## Good to go\n",
    "def load_and_process_drug_consumption_data(path):\n",
    "\n",
    "    # path = os.path.join(\"data\", \"drug_consumption.csv\")\n",
    "    path = \"../data/drug_consumption.csv\"\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "    \n",
    "    # convert to 4 classes\n",
    "    df = df[df.columns[1:]]\n",
    "    df = df.replace(\n",
    "        {\n",
    "            \"cannabis\": {\n",
    "                \"CL0\": \"never_used\",\n",
    "                \"CL1\": \"not_in_last_year\",\n",
    "                \"CL2\": \"not_in_last_year\",\n",
    "                \"CL3\": \"used_in_last_year\",\n",
    "                \"CL4\": \"used_in_last_year\",\n",
    "                \"CL5\": \"used_in_last_week\",\n",
    "                \"CL6\": \"used_in_last_week\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    educated_cat = {\n",
    "        \"University degree\",\n",
    "        \"Masters degree\",\n",
    "        \"Doctorate degree\",\n",
    "        \"Professional certificate/ diploma\"\n",
    "    }\n",
    "    \n",
    "    df[\"education\"] = df[\"education\"].apply(lambda x: 1 if x in educated_cat else 0)\n",
    "    \n",
    "    # changing to numerical representation\n",
    "    label_encoder = LabelEncoder()\n",
    "    # df[\"age\"] = label_encoder.fit_transform(df[\"age\"]) \n",
    "    df[\"country\"] = label_encoder.fit_transform(df[\"country\"])\n",
    "    df[\"ethnicity\"] = label_encoder.fit_transform(df[\"ethnicity\"])\n",
    "    df[\"cannabis\"] = label_encoder.fit_transform(df[\"cannabis\"])\n",
    "\n",
    "    \n",
    "    \n",
    "    df[\"gender\"] = df[\"gender\"].apply(lambda x: 1 if x == \"M\" else 0)\n",
    "    \n",
    "    X = df[df.columns[1:12]]\n",
    "    Y = df[\"cannabis\"].to_numpy()\n",
    "    S = df[\"education\"].to_numpy()\n",
    "    X = X.drop(columns = [\"education\"])\n",
    "\n",
    "    return X, Y, S\n",
    "\n",
    "multi_main(\"ANYTHING\", \"drug\", lambda_adv=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d2a34-5537-464d-bfa2-145aca328906",
   "metadata": {},
   "source": [
    "#### Compas Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9792f0-69b2-4b91-b491-c5acfac2ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COME BACK AND DO IF WANT TO IMPLEMENT WHERE SENSITIVE FEATURE IS NOT JUST BINARY\n",
    "def load_and_preprocess_compas_data_multi_cat(data_url):\n",
    "    \"\"\"\n",
    "    Try the algorithm, except instead of binary sensitive feature, it multi classes (so not just Aferican American)? \n",
    "    \n",
    "    Download and preprocess the COMPAS dataset.\n",
    "\n",
    "    We assume the dataset contains, among others, the following columns:\n",
    "      - 'age'\n",
    "      - 'race'\n",
    "      - 'priors_count'\n",
    "      - 'juv_fel_count'\n",
    "      - 'juv_misd_count'\n",
    "      - 'juv_other_count'\n",
    "      - 'two_year_recid'\n",
    "\n",
    "    Features (X): We select a few numerical features.\n",
    "    Observed Label (Y): Use 'two_year_recid' as a binary label (0/1).\n",
    "    Protected Attribute (S): Use 'race'. Here we binarize race so that:\n",
    "         African‑American  → 1\n",
    "         all other races  → 0.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_url)\n",
    "    # Drop rows with missing values in the selected columns.\n",
    "    data = data.dropna(subset=[\"age\", \"race\", \"priors_count\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\", \"two_year_recid\"])\n",
    "\n",
    "    # Observed label: two_year_recid (already 0/1)\n",
    "    Y = data[\"two_year_recid\"].values\n",
    "\n",
    "    # Sensitive attribute: race. We set S=1 if race is African-American, else 0.\n",
    "    S = (data[\"race\"] == \"African-American\").astype(int).values\n",
    "\n",
    "    # Features: use a subset of numerical features.\n",
    "    feature_cols = [\"age\", \"priors_count\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\"]\n",
    "    X = data[feature_cols].copy().astype(np.float32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X.values)\n",
    "\n",
    "    return X, Y, S\n",
    "\n",
    "# URL for the ProPublica COMPAS dataset\n",
    "compas_data_url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "# You can adjust lambda_adv as desired (e.g., lambda_adv=15.5 as in your German data experiment)\n",
    "main_compas(compas_data_url, lambda_adv=3.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
