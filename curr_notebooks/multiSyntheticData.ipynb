{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf22c554-b495-4551-ae60-0381413fb23b",
   "metadata": {},
   "source": [
    "#### Plan for synthetic data \n",
    "1. Create synthetic datasets\n",
    "2. Run synthetic unbiased fair synthetic dataset\n",
    "3. add in error + bias into Y-train, make it rely on S\n",
    "4. run pipeline on new data and try to get it to meet the same baseline as the original dataset for demographic parity + accuracy.\n",
    "\n",
    "#### Additional work \n",
    "hypertune for lambda so that accuracy and demographic parity are well balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24672b93-3c9d-4e08-98b2-b6a106a323bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 14:56:25.314270: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-01 14:56:25.314303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-01 14:56:25.315935: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-01 14:56:25.324401: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# !pip install fairlearn\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Flatten, \n",
    "    MaxPooling2D, BatchNormalization, Dropout\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Flatten, \n",
    "    MaxPooling2D, BatchNormalization, Dropout, Concatenate\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler\n",
    ")\n",
    "\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9af067-aa00-404e-9bbb-9fa55fd4367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Custom Gradient Reversal Layer\n",
    "# -------------------------------\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x, lambda_):\n",
    "    def grad(dy):\n",
    "        return -lambda_ * dy, None # reverses direction of gradient \n",
    "    return x, grad\n",
    "\n",
    "# custom Keras layer\n",
    "\"\"\"\n",
    "Layer is used to ensure that the feature representation are independent of a sensitive attribute\n",
    "- feature extract learns normally in the forward pass\n",
    "- reversing gradients of classifier that tries to predict the sensitive attribute during backpropagation -- stops feature extractor from encoding sensitive information\n",
    "\"\"\"\n",
    "class GradientReversalLayer(tf.keras.layers.Layer): \n",
    "    def __init__(self, lambda_=1.0, **kwargs):\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "        self.lambda_ = lambda_ # strength of gradient reversal\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x, self.lambda_)\n",
    "\n",
    "# -------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# -------------------------------\n",
    "def set_seed(seed_num):\n",
    "    random.seed(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98a279a6-a67d-4bab-8049-18a4eb1ab4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Adversarial Debiasing Model\n",
    "# -------------------------------\n",
    "def build_adversarial_model(input_dim, lambda_adv=1.0):\n",
    "    \"\"\"\n",
    "    Build an adversarial debiasing model that learns pseudo‑labels Y' from X.\n",
    "\n",
    "    Architecture:\n",
    "      - Main branch (encoder): from X, several dense layers produce a latent pseudo‑label pseudo_Y (via softmax for multi-class).\n",
    "      - Adversary branch: pseudo_Y is passed through a Gradient Reversal Layer and then dense layers predict S.\n",
    "      - Decoder branch: concatenates pseudo_Y and the one-hot sensitive attribute S to predict the observed label Y.\n",
    "\n",
    "    Losses:\n",
    "      - For the main branch, categorical crossentropy between observed Y and pseudo_Y (and Y_pred).\n",
    "      - For the adversary branch, categorical crossentropy to predict S.\n",
    "\n",
    "    Returns a compiled Keras model that takes inputs X and S (one-hot encoded) and outputs:\n",
    "      [pseudo_Y, S_pred, Y_pred].\n",
    "    \"\"\"\n",
    "    X_input = tf.keras.Input(shape=(input_dim,), name=\"X\")\n",
    "    S_input = tf.keras.Input(shape=(2,), name=\"S\")  # one-hot encoded S\n",
    "\n",
    "    # Main branch: Encoder for pseudo-label.\n",
    "    h = Dense(64, activation='relu')(X_input)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Dense(32, activation='relu')(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    pseudo_Y = Dense(4, activation='softmax', name=\"pseudo_Y\")(h)  # Multi-class pseudo-labels\n",
    "\n",
    "    # Adversary branch: from pseudo_Y, with GRL.\n",
    "    \"\"\"\n",
    "    This prevents pseudo_Y from encoding information about S.\n",
    "    - The adversary will try to predict S from pseudo_Y (fair label). If it can accurately predict S, then pseudo_Y still encodes S (which we want to prevent).\n",
    "    - Use the gradient reversal layer to counteract this effect.\n",
    "    \"\"\"\n",
    "    grl = GradientReversalLayer(lambda_=lambda_adv)(pseudo_Y)\n",
    "    a = Dense(32, activation='relu')(grl)\n",
    "    a = BatchNormalization()(a)\n",
    "    S_pred = Dense(2, activation='softmax', name=\"S_pred\")(a)  # Binary classification for S\n",
    "\n",
    "    # Decoder branch: combine pseudo_Y and S to predict observed Y.\n",
    "    \"\"\"\n",
    "    Y depends on both pseudo_Y and S.\n",
    "    - Predict the final observed label Y using both pseudo_Y and S.\n",
    "    - Ensures pseudo_Y is not directly influenced by S while still allowing S to contribute to Y_pred.\n",
    "    - Maintains fair dependencies while eliminating unfair ones.\n",
    "    \"\"\"\n",
    "    concat = Concatenate()([pseudo_Y, S_input])\n",
    "    d = Dense(16, activation='relu')(concat)\n",
    "    d = BatchNormalization()(d)\n",
    "    Y_pred = Dense(4, activation='softmax', name=\"Y_pred\")(d)  # Multi-class output\n",
    "\n",
    "    model = tf.keras.Model(inputs=[X_input, S_input],\n",
    "                           outputs=[pseudo_Y, S_pred, Y_pred])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  loss={\"pseudo_Y\": \"categorical_crossentropy\",  # Changed to categorical\n",
    "                        \"S_pred\": \"categorical_crossentropy\",\n",
    "                        \"Y_pred\": \"categorical_crossentropy\"},  # Changed to categorical\n",
    "                  loss_weights={\"pseudo_Y\": 1.0, \"S_pred\": lambda_adv, \"Y_pred\": 1.0},\n",
    "                  metrics={\"pseudo_Y\": \"accuracy\",\n",
    "                           \"S_pred\": \"accuracy\",\n",
    "                           \"Y_pred\": \"accuracy\"})  # Multi-class accuracy\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdce495e-4c4f-4024-81ae-ec51509beaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Manual Fairness Metrics\n",
    "# -------------------------------\n",
    "def compute_fairness_metrics_manual(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Compute fairness metrics manually for multi-class classification.\n",
    "    \n",
    "    y_true: ground-truth labels (1-D numpy array with class values 0,1,2,3).\n",
    "    y_pred: predicted class probabilities (2-D numpy array with shape (n_samples, 4)).\n",
    "    sensitive_features: 1-D numpy array (0 or 1).\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - Demographic parity difference (absolute difference in selection rates across groups).\n",
    "      - Equalized odds difference (average difference in per-class TPR and FPR across groups).\n",
    "      - Selection rates per group.\n",
    "      - Group-wise accuracy.\n",
    "    \"\"\"\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)  # Convert softmax probabilities to class predictions\n",
    "    groups = np.unique(sensitive_features)\n",
    "\n",
    "    # Demographic Parity (Class-wise selection rate difference)\n",
    "    \"\"\"\n",
    "    All groups (from the sensitive feature) should receive predictions at similar rates for each class.\n",
    "    Computes the absolute difference in selection rates per class between S=0 and S=1.\n",
    "    \"\"\"\n",
    "    pos_rates = {g: np.zeros(4) for g in groups}  # Store selection rates for each class (0,1,2,3)\n",
    "    for g in groups:\n",
    "        for c in range(4):  # Iterate through classes\n",
    "            pos_rates[g][c] = np.mean(y_pred_class[sensitive_features == g] == c)\n",
    "\n",
    "    dp_diff = np.mean(np.abs(pos_rates[0] - pos_rates[1]))  # Average difference across all classes\n",
    "\n",
    "    # Equalized Odds (Per-Class TPR and FPR comparison across groups)\n",
    "    \"\"\"\n",
    "    Ensuring that different groups in the sensitive feature have similar TPR and FPR rates for each class.\n",
    "    This prevents the model from favoring certain groups in errors.\n",
    "    \"\"\"\n",
    "    metrics = {g: {'tpr': np.zeros(4), 'fpr': np.zeros(4)} for g in groups}\n",
    "    \n",
    "    for g in groups:\n",
    "        mask = (sensitive_features == g)\n",
    "        y_true_g = y_true[mask]\n",
    "        y_pred_g = y_pred_class[mask]\n",
    "\n",
    "        for c in range(4):\n",
    "            tp = np.sum((y_pred_g == c) & (y_true_g == c))\n",
    "            fn = np.sum((y_pred_g != c) & (y_true_g == c))\n",
    "            fp = np.sum((y_pred_g == c) & (y_true_g != c))\n",
    "            tn = np.sum((y_pred_g != c) & (y_true_g != c))\n",
    "            \n",
    "            tpr = tp / (tp + fn + 1e-8)  # True Positive Rate for class c\n",
    "            fpr = fp / (fp + tn + 1e-8)  # False Positive Rate for class c\n",
    "            \n",
    "            metrics[g]['tpr'][c] = tpr\n",
    "            metrics[g]['fpr'][c] = fpr\n",
    "\n",
    "    # Compute the Equalized Odds Difference as the average absolute difference in TPR and FPR across all classes\n",
    "    eo_diff = np.mean(np.abs(metrics[0]['tpr'] - metrics[1]['tpr'])) + np.mean(np.abs(metrics[0]['fpr'] - metrics[1]['fpr']))\n",
    "\n",
    "    # Selection Rate Per Group\n",
    "    \"\"\"\n",
    "    Proportion of samples predicted as each class for each group.\n",
    "    A higher selection rate for certain classes in one group may indicate bias.\n",
    "    \"\"\"\n",
    "    sel_rate = {g: pos_rates[g].tolist() for g in groups}\n",
    "\n",
    "    # Group-wise Accuracy\n",
    "    \"\"\"\n",
    "    For each group in the sensitive feature, compute the accuracy to ensure consistent performance.\n",
    "    \"\"\"\n",
    "    group_acc = {}\n",
    "    for g in groups:\n",
    "        mask = (sensitive_features == g)\n",
    "        group_acc[g] = accuracy_score(y_true[mask], y_pred_class[mask])\n",
    "\n",
    "    return {\n",
    "        \"demographic_parity_difference\": dp_diff,\n",
    "        \"equalized_odds_difference\": eo_diff,\n",
    "        \"selection_rate\": sel_rate,\n",
    "        \"group_accuracy\": group_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215ef3cd-b252-45a1-a259-d6b4dd09001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Plotting Function\n",
    "# -------------------------------\n",
    "def plot_comparison(metrics_baseline, metrics_fair):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - metrics_baseline: Dictionary storing evaluation metrics for the baseline model.\n",
    "    - metrics_fair: Dictionary storing evaluation metrics for the fair model.\n",
    "\n",
    "    Plots a comparison of key fairness and performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    models = ['Baseline', 'Fair']\n",
    "    \n",
    "    # Ensure AUC is correctly computed for multi-class\n",
    "    aucs = [metrics_baseline.get('auc', 0), metrics_fair.get('auc', 0)]\n",
    "    accs = [metrics_baseline.get('accuracy', 0), metrics_fair.get('accuracy', 0)]\n",
    "    dp_diff = [metrics_baseline.get(\"demographic_parity_difference\", 0), \n",
    "               metrics_fair.get(\"demographic_parity_difference\", 0)]\n",
    "    eo_diff = [metrics_baseline.get(\"equalized_odds_difference\", 0), \n",
    "               metrics_fair.get(\"equalized_odds_difference\", 0)]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # Multi-Class AUC (Make sure it's properly calculated elsewhere)\n",
    "    axs[0,0].bar(models, aucs, color=['blue', 'green'])\n",
    "    axs[0,0].set_title('Multi-Class AUC')\n",
    "    axs[0,0].set_ylim([0, 1])\n",
    "\n",
    "    # Accuracy comparison\n",
    "    axs[0,1].bar(models, accs, color=['blue', 'green'])\n",
    "    axs[0,1].set_title('Accuracy')\n",
    "    axs[0,1].set_ylim([0, 1])\n",
    "\n",
    "    # Demographic Parity Difference (Lower is better)\n",
    "    axs[1,0].bar(models, dp_diff, color=['orange', 'purple'])\n",
    "    axs[1,0].set_title('Demographic Parity Difference')\n",
    "\n",
    "    # Equalized Odds Difference (Lower is better)\n",
    "    axs[1,1].bar(models, eo_diff, color=['orange', 'purple'])\n",
    "    axs[1,1].set_title('Equalized Odds Difference')\n",
    "\n",
    "    plt.suptitle(\"Comparison: Baseline (X → Y) vs. Fair (X → Y') Model\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb64d7f-b1bf-44e6-8a2e-82ec01d85ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=5000, n_features=30, bias_factor=0.3, noise_level=0.1, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate Sensitive Attribute S ~ Binomial(1, 0.5)\n",
    "    S = np.random.binomial(1, 0.5, size=n_samples)\n",
    "\n",
    "    # Generate Features X: Some function of S + Gaussian noise\n",
    "    X = np.random.randn(n_samples, n_features) + 0.5 * S[:, np.newaxis]\n",
    "\n",
    "    # Generate True Labels Y (linear function of X + noise)\n",
    "    true_weights = np.random.randn(n_features)\n",
    "    Y_continuous = X @ true_weights + np.random.normal(0, noise_level, size=n_samples)\n",
    "\n",
    "    # Convert Y into four discrete categories\n",
    "    bins = np.percentile(Y_continuous, [25, 50, 75])  # 3 cut points create 4 bins\n",
    "    Y = np.digitize(Y_continuous, bins=bins)  # Bins index (0,1,2,3)\n",
    "\n",
    "    # Ensure Y is properly categorical\n",
    "    Y = Y.astype(int)\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, Y_train_obs, Y_test_obs, S_train, S_test = train_test_split(\n",
    "        X, Y, S, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, Y_train_obs, Y_test_obs, S_train, S_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db287ff-63dd-4c5d-9aa9-324e31b26826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_bias(bias_factor=0.4, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    X_train, X_test, Y_train_raw, Y_test_raw, S_train, S_test = generate_synthetic_data()\n",
    "\n",
    "    def apply_bias(Y, S):\n",
    "        flip_mask = np.random.rand(len(Y)) < bias_factor  # Generate a flip mask for this dataset\n",
    "        Y_biased = Y.copy()\n",
    "\n",
    "        # Introduce bias by shifting class labels based on S\n",
    "        for i in range(len(Y_biased)):\n",
    "            if flip_mask[i]:\n",
    "                if S[i] == 1:  # Favoring higher classes for S=1\n",
    "                    if Y_biased[i] < 3:  # Avoid exceeding class range\n",
    "                        Y_biased[i] += 1  \n",
    "                elif S[i] == 0:  # Favoring lower classes for S=0\n",
    "                    if Y_biased[i] > 0:  # Avoid going below class range\n",
    "                        Y_biased[i] -= 1  \n",
    "\n",
    "        return Y_biased\n",
    "\n",
    "    Y_train_biased = apply_bias(Y_train_raw, S_train)\n",
    "    Y_test_biased = apply_bias(Y_test_raw, S_test)\n",
    "\n",
    "    return Y_train_biased, Y_test_biased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86fc6588-04a1-4918-bf7c-6f8466a2da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: AUC: 0.9984, Accuracy: 0.9660, Demographic Parity Difference: 0.0236, Equalized Odds Difference: 0.0471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def run_biased_logistic(X_train, Y_train_biased_pred, X_test, Y_test_biased_pred, Y_train_raw, Y_test_raw, S_train, S_test): \n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "    clf.fit(X_train, Y_train_biased_pred)\n",
    "\n",
    "    preds_proba = clf.predict_proba(X_test)  # Multi-class probability predictions\n",
    "    preds_class = clf.predict(X_test)  # Class predictions\n",
    "\n",
    "    # Compute multi-class AUC\n",
    "    auc = roc_auc_score(to_categorical(Y_test_raw, num_classes=4), preds_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "\n",
    "    # Compute multi-class accuracy\n",
    "    acc = accuracy_score(Y_test_raw, preds_class)\n",
    "\n",
    "    # Compute fairness metrics\n",
    "    fairness = compute_fairness_metrics_manual(Y_test_raw, preds_proba, sensitive_features=S_test)\n",
    "\n",
    "    return auc, acc, fairness\n",
    "\n",
    "\n",
    "def run_unbiased_logistic(): \n",
    "    X_train, X_test, Y_train_raw, Y_test_raw, S_train, S_test = generate_synthetic_data()  # Y is multi-class, S is binary\n",
    "    \n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "    clf.fit(X_train, Y_train_raw)\n",
    "\n",
    "    preds_proba = clf.predict_proba(X_test)  # Multi-class probability predictions\n",
    "    preds_class = clf.predict(X_test)  # Class predictions\n",
    "\n",
    "    # Compute multi-class AUC\n",
    "    auc = roc_auc_score(to_categorical(Y_test_raw, num_classes=4), preds_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "\n",
    "    # Compute multi-class accuracy\n",
    "    acc = accuracy_score(Y_test_raw, preds_class)\n",
    "\n",
    "    # Compute fairness metrics\n",
    "    fairness = compute_fairness_metrics_manual(Y_test_raw, preds_proba, sensitive_features=S_test)\n",
    "\n",
    "    dp_diff = fairness[\"demographic_parity_difference\"]\n",
    "    eo_diff = fairness[\"equalized_odds_difference\"]\n",
    "\n",
    "    print(f\"Baseline: AUC: {auc:.4f}, Accuracy: {acc:.4f}, Demographic Parity Difference: {dp_diff:.4f}, Equalized Odds Difference: {eo_diff:.4f}\")\n",
    "\n",
    "# Run the unbiased logistic regression model\n",
    "run_unbiased_logistic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9934d0d9-03c6-4480-a9d2-90283ba9e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Main Function: Comparison and Visualization\n",
    "# -------------------------------\n",
    "def main_synthetic(lambda_adv=1.0, epochs=64, batch_size=128):\n",
    "    set_seed(42)\n",
    "\n",
    "    # Generate multi-class synthetic dataset (Y has 4 classes, S is binary)\n",
    "    X_train, X_test, Y_train_raw, Y_test_raw, S_train, S_test = generate_synthetic_data()\n",
    "\n",
    "    # Inject bias into Y_train and Y_test\n",
    "    Y_train_biased, Y_test_biased = inject_bias(bias_factor=0.3, seed=42)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # One-hot encode S for adversarial model training.\n",
    "    S_train_oh = tf.keras.utils.to_categorical(S_train, num_classes=2)\n",
    "    S_test_oh  = tf.keras.utils.to_categorical(S_test, num_classes=2)\n",
    "\n",
    "    ### 1. Train adversarial debiasing model (X → Y' with adversary)\n",
    "    print(\"\\nTraining adversarial model (X → Y' with adversary) ...\")\n",
    "    adv_model = build_adversarial_model(input_dim, lambda_adv=lambda_adv)\n",
    "\n",
    "    # Convert Y to one-hot encoding for training\n",
    "    Y_train_biased_oh = tf.keras.utils.to_categorical(Y_train_biased, num_classes=4)\n",
    "    Y_test_biased_oh = tf.keras.utils.to_categorical(Y_test_biased, num_classes=4)\n",
    "\n",
    "    # Train the adversarial model\n",
    "    adv_model.fit([X_train, S_train_oh],\n",
    "                  {\"pseudo_Y\": Y_train_biased_oh, \"S_pred\": S_train_oh, \"Y_pred\": Y_train_biased_oh},\n",
    "                  epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Get predictions from the adversarial model\n",
    "    pseudo_Y_train, _, Y_pred_train = adv_model.predict([X_train, S_train_oh])\n",
    "    pseudo_Y_test,  _, Y_pred_test = adv_model.predict([X_test, S_test_oh])\n",
    "\n",
    "    # Convert softmax outputs to class predictions\n",
    "    pseudo_Y_train_class = np.argmax(pseudo_Y_train, axis=1)  # Multi-class prediction\n",
    "    pseudo_Y_test_class = np.argmax(pseudo_Y_test, axis=1)  # Multi-class prediction\n",
    "\n",
    "    print(\"\\nPseudo-label statistics (training):\")\n",
    "    for g in np.unique(S_train):\n",
    "        mask = (S_train == g)\n",
    "        print(f\"Group {g} pseudo-class distribution: {np.bincount(pseudo_Y_train_class[mask], minlength=4)}\") \n",
    "\n",
    "    ### 2. Train baseline logistic regression model on observed Y (X → Y)\n",
    "    print(\"\\nTraining baseline [BIASED] logistic regression classifier (X → Y)...\")\n",
    "    baseline_auc, baseline_acc, baseline_fairness = run_biased_logistic(\n",
    "        X_train, Y_train_biased, X_test, Y_test_biased, Y_train_raw, Y_test_raw, S_train, S_test\n",
    "    )\n",
    "\n",
    "    ### 3. Train fair logistic regression model on pseudo-labels (X → Y')\n",
    "    print(\"\\nTraining fair logistic regression classifier (X → Y') using pseudo-labels...\")\n",
    "    fair_auc, fair_acc, fair_fairness = run_biased_logistic(\n",
    "        X_train, pseudo_Y_train_class, X_test, pseudo_Y_test_class, Y_train_raw, Y_test_raw, S_train, S_test\n",
    "    )\n",
    "\n",
    "    # Aggregate metrics for plotting.\n",
    "    metrics_baseline = {\n",
    "        \"auc\": baseline_auc,\n",
    "        \"accuracy\": baseline_acc,\n",
    "        \"demographic_parity_difference\": baseline_fairness[\"demographic_parity_difference\"],\n",
    "        \"equalized_odds_difference\": baseline_fairness[\"equalized_odds_difference\"]\n",
    "    }\n",
    "    metrics_fair = {\n",
    "        \"auc\": fair_auc,\n",
    "        \"accuracy\": fair_acc,\n",
    "        \"demographic_parity_difference\": fair_fairness[\"demographic_parity_difference\"],\n",
    "        \"equalized_odds_difference\": fair_fairness[\"equalized_odds_difference\"]\n",
    "    }\n",
    "\n",
    "    print(\"\\nBaseline Logistic Regression (X → Y) Evaluation:\")\n",
    "    print(f\"AUC: {baseline_auc:.4f}, Accuracy: {baseline_acc:.4f}\")\n",
    "    print(\"Fairness metrics:\", baseline_fairness)\n",
    "\n",
    "    print(\"\\nFair Logistic Regression (X → Y') Evaluation (compared to observed Y):\")\n",
    "    print(f\"AUC: {fair_auc:.4f}, Accuracy: {fair_acc:.4f}\")\n",
    "    print(\"Fairness metrics:\", fair_fairness)\n",
    "\n",
    "    # Plot comparison.\n",
    "    plot_comparison(metrics_baseline, metrics_fair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a7324-619d-470b-9ec7-f1b4bf6d984f",
   "metadata": {},
   "source": [
    "### Application on Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a8cb1-26fd-47ae-ba82-01642fb05249",
   "metadata": {},
   "source": [
    "#### Synthetic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53f08737-2fab-439f-aef0-27536aa9cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training adversarial model (X → Y' with adversary) ...\n",
      "Epoch 1/64\n",
      "32/32 [==============================] - 3s 10ms/step - loss: 4.5339 - pseudo_Y_loss: 1.8661 - S_pred_loss: 0.9047 - Y_pred_loss: 1.7630 - pseudo_Y_accuracy: 0.2612 - S_pred_accuracy: 0.5255 - Y_pred_accuracy: 0.2645\n",
      "Epoch 2/64\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 4.4853 - pseudo_Y_loss: 1.8169 - S_pred_loss: 0.9266 - Y_pred_loss: 1.7418 - pseudo_Y_accuracy: 0.2695 - S_pred_accuracy: 0.4955 - Y_pred_accuracy: 0.2615\n",
      "Epoch 3/64\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 4.4200 - pseudo_Y_loss: 1.7755 - S_pred_loss: 0.9205 - Y_pred_loss: 1.7240 - pseudo_Y_accuracy: 0.2812 - S_pred_accuracy: 0.4655 - Y_pred_accuracy: 0.2635\n",
      "Epoch 4/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 4.3301 - pseudo_Y_loss: 1.7372 - S_pred_loss: 0.8974 - Y_pred_loss: 1.6955 - pseudo_Y_accuracy: 0.2878 - S_pred_accuracy: 0.4582 - Y_pred_accuracy: 0.2632\n",
      "Epoch 5/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 4.2347 - pseudo_Y_loss: 1.6979 - S_pred_loss: 0.8667 - Y_pred_loss: 1.6701 - pseudo_Y_accuracy: 0.2973 - S_pred_accuracy: 0.4540 - Y_pred_accuracy: 0.2710\n",
      "Epoch 6/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 4.1356 - pseudo_Y_loss: 1.6606 - S_pred_loss: 0.8279 - Y_pred_loss: 1.6471 - pseudo_Y_accuracy: 0.3067 - S_pred_accuracy: 0.4627 - Y_pred_accuracy: 0.2745\n",
      "Epoch 7/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 4.0423 - pseudo_Y_loss: 1.6224 - S_pred_loss: 0.7927 - Y_pred_loss: 1.6272 - pseudo_Y_accuracy: 0.3162 - S_pred_accuracy: 0.4810 - Y_pred_accuracy: 0.2817\n",
      "Epoch 8/64\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 3.9500 - pseudo_Y_loss: 1.5812 - S_pred_loss: 0.7674 - Y_pred_loss: 1.6014 - pseudo_Y_accuracy: 0.3230 - S_pred_accuracy: 0.4925 - Y_pred_accuracy: 0.2895\n",
      "Epoch 9/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 3.8740 - pseudo_Y_loss: 1.5445 - S_pred_loss: 0.7481 - Y_pred_loss: 1.5814 - pseudo_Y_accuracy: 0.3395 - S_pred_accuracy: 0.5042 - Y_pred_accuracy: 0.2988\n",
      "Epoch 10/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 3.8045 - pseudo_Y_loss: 1.5037 - S_pred_loss: 0.7399 - Y_pred_loss: 1.5609 - pseudo_Y_accuracy: 0.3465 - S_pred_accuracy: 0.4997 - Y_pred_accuracy: 0.3050\n",
      "Epoch 11/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 3.7378 - pseudo_Y_loss: 1.4655 - S_pred_loss: 0.7338 - Y_pred_loss: 1.5385 - pseudo_Y_accuracy: 0.3602 - S_pred_accuracy: 0.4980 - Y_pred_accuracy: 0.3115\n",
      "Epoch 12/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 3.6676 - pseudo_Y_loss: 1.4267 - S_pred_loss: 0.7322 - Y_pred_loss: 1.5087 - pseudo_Y_accuracy: 0.3697 - S_pred_accuracy: 0.5090 - Y_pred_accuracy: 0.3288\n",
      "Epoch 13/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 3.6057 - pseudo_Y_loss: 1.3944 - S_pred_loss: 0.7333 - Y_pred_loss: 1.4780 - pseudo_Y_accuracy: 0.3868 - S_pred_accuracy: 0.4947 - Y_pred_accuracy: 0.3305\n",
      "Epoch 14/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 3.5471 - pseudo_Y_loss: 1.3626 - S_pred_loss: 0.7321 - Y_pred_loss: 1.4524 - pseudo_Y_accuracy: 0.3947 - S_pred_accuracy: 0.4885 - Y_pred_accuracy: 0.3480\n",
      "Epoch 15/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 3.4868 - pseudo_Y_loss: 1.3314 - S_pred_loss: 0.7278 - Y_pred_loss: 1.4276 - pseudo_Y_accuracy: 0.4090 - S_pred_accuracy: 0.4897 - Y_pred_accuracy: 0.3505\n",
      "Epoch 16/64\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 3.4164 - pseudo_Y_loss: 1.2958 - S_pred_loss: 0.7263 - Y_pred_loss: 1.3943 - pseudo_Y_accuracy: 0.4235 - S_pred_accuracy: 0.4760 - Y_pred_accuracy: 0.3658\n",
      "Epoch 17/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 3.3575 - pseudo_Y_loss: 1.2676 - S_pred_loss: 0.7249 - Y_pred_loss: 1.3649 - pseudo_Y_accuracy: 0.4360 - S_pred_accuracy: 0.4755 - Y_pred_accuracy: 0.3803\n",
      "Epoch 18/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 3.3012 - pseudo_Y_loss: 1.2441 - S_pred_loss: 0.7184 - Y_pred_loss: 1.3386 - pseudo_Y_accuracy: 0.4470 - S_pred_accuracy: 0.4780 - Y_pred_accuracy: 0.3875\n",
      "Epoch 19/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 3.2441 - pseudo_Y_loss: 1.2153 - S_pred_loss: 0.7191 - Y_pred_loss: 1.3097 - pseudo_Y_accuracy: 0.4588 - S_pred_accuracy: 0.4720 - Y_pred_accuracy: 0.3907\n",
      "Epoch 20/64\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 3.1794 - pseudo_Y_loss: 1.1917 - S_pred_loss: 0.7113 - Y_pred_loss: 1.2764 - pseudo_Y_accuracy: 0.4703 - S_pred_accuracy: 0.4857 - Y_pred_accuracy: 0.4110\n",
      "Epoch 21/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 3.1289 - pseudo_Y_loss: 1.1692 - S_pred_loss: 0.7103 - Y_pred_loss: 1.2494 - pseudo_Y_accuracy: 0.4737 - S_pred_accuracy: 0.4825 - Y_pred_accuracy: 0.4115\n",
      "Epoch 22/64\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 3.0705 - pseudo_Y_loss: 1.1458 - S_pred_loss: 0.7078 - Y_pred_loss: 1.2169 - pseudo_Y_accuracy: 0.4913 - S_pred_accuracy: 0.4942 - Y_pred_accuracy: 0.4300\n",
      "Epoch 23/64\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 3.0223 - pseudo_Y_loss: 1.1245 - S_pred_loss: 0.7053 - Y_pred_loss: 1.1925 - pseudo_Y_accuracy: 0.4908 - S_pred_accuracy: 0.4927 - Y_pred_accuracy: 0.4595\n",
      "Epoch 24/64\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 2.9845 - pseudo_Y_loss: 1.1086 - S_pred_loss: 0.7077 - Y_pred_loss: 1.1682 - pseudo_Y_accuracy: 0.5070 - S_pred_accuracy: 0.4905 - Y_pred_accuracy: 0.4863\n",
      "Epoch 25/64\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.9356 - pseudo_Y_loss: 1.0874 - S_pred_loss: 0.7047 - Y_pred_loss: 1.1434 - pseudo_Y_accuracy: 0.5135 - S_pred_accuracy: 0.5000 - Y_pred_accuracy: 0.5017\n",
      "Epoch 26/64\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.8940 - pseudo_Y_loss: 1.0715 - S_pred_loss: 0.7038 - Y_pred_loss: 1.1187 - pseudo_Y_accuracy: 0.5163 - S_pred_accuracy: 0.4952 - Y_pred_accuracy: 0.5213\n",
      "Epoch 27/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.8575 - pseudo_Y_loss: 1.0571 - S_pred_loss: 0.7036 - Y_pred_loss: 1.0968 - pseudo_Y_accuracy: 0.5270 - S_pred_accuracy: 0.4950 - Y_pred_accuracy: 0.5288\n",
      "Epoch 28/64\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.8248 - pseudo_Y_loss: 1.0444 - S_pred_loss: 0.7030 - Y_pred_loss: 1.0773 - pseudo_Y_accuracy: 0.5343 - S_pred_accuracy: 0.4955 - Y_pred_accuracy: 0.5415\n",
      "Epoch 29/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.7895 - pseudo_Y_loss: 1.0322 - S_pred_loss: 0.7010 - Y_pred_loss: 1.0563 - pseudo_Y_accuracy: 0.5380 - S_pred_accuracy: 0.5045 - Y_pred_accuracy: 0.5500\n",
      "Epoch 30/64\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.7488 - pseudo_Y_loss: 1.0133 - S_pred_loss: 0.7026 - Y_pred_loss: 1.0330 - pseudo_Y_accuracy: 0.5458 - S_pred_accuracy: 0.4933 - Y_pred_accuracy: 0.5600\n",
      "Epoch 31/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.7146 - pseudo_Y_loss: 0.9984 - S_pred_loss: 0.6991 - Y_pred_loss: 1.0170 - pseudo_Y_accuracy: 0.5558 - S_pred_accuracy: 0.5040 - Y_pred_accuracy: 0.5655\n",
      "Epoch 32/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.6875 - pseudo_Y_loss: 0.9888 - S_pred_loss: 0.6991 - Y_pred_loss: 0.9995 - pseudo_Y_accuracy: 0.5610 - S_pred_accuracy: 0.5085 - Y_pred_accuracy: 0.5755\n",
      "Epoch 33/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.6547 - pseudo_Y_loss: 0.9743 - S_pred_loss: 0.6984 - Y_pred_loss: 0.9820 - pseudo_Y_accuracy: 0.5700 - S_pred_accuracy: 0.5105 - Y_pred_accuracy: 0.5788\n",
      "Epoch 34/64\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.6189 - pseudo_Y_loss: 0.9597 - S_pred_loss: 0.6987 - Y_pred_loss: 0.9606 - pseudo_Y_accuracy: 0.5815 - S_pred_accuracy: 0.5102 - Y_pred_accuracy: 0.5910\n",
      "Epoch 35/64\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.6102 - pseudo_Y_loss: 0.9568 - S_pred_loss: 0.6983 - Y_pred_loss: 0.9550 - pseudo_Y_accuracy: 0.5755 - S_pred_accuracy: 0.5142 - Y_pred_accuracy: 0.5910\n",
      "Epoch 36/64\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.5782 - pseudo_Y_loss: 0.9445 - S_pred_loss: 0.6993 - Y_pred_loss: 0.9345 - pseudo_Y_accuracy: 0.5770 - S_pred_accuracy: 0.5058 - Y_pred_accuracy: 0.5947\n",
      "Epoch 37/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.5561 - pseudo_Y_loss: 0.9363 - S_pred_loss: 0.6972 - Y_pred_loss: 0.9226 - pseudo_Y_accuracy: 0.5807 - S_pred_accuracy: 0.5170 - Y_pred_accuracy: 0.6090\n",
      "Epoch 38/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.5309 - pseudo_Y_loss: 0.9258 - S_pred_loss: 0.6968 - Y_pred_loss: 0.9082 - pseudo_Y_accuracy: 0.5928 - S_pred_accuracy: 0.5138 - Y_pred_accuracy: 0.6108\n",
      "Epoch 39/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.5082 - pseudo_Y_loss: 0.9170 - S_pred_loss: 0.6976 - Y_pred_loss: 0.8936 - pseudo_Y_accuracy: 0.5965 - S_pred_accuracy: 0.5132 - Y_pred_accuracy: 0.6085\n",
      "Epoch 40/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4892 - pseudo_Y_loss: 0.9096 - S_pred_loss: 0.6973 - Y_pred_loss: 0.8823 - pseudo_Y_accuracy: 0.5953 - S_pred_accuracy: 0.5100 - Y_pred_accuracy: 0.6155\n",
      "Epoch 41/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4708 - pseudo_Y_loss: 0.9019 - S_pred_loss: 0.6963 - Y_pred_loss: 0.8726 - pseudo_Y_accuracy: 0.6020 - S_pred_accuracy: 0.5110 - Y_pred_accuracy: 0.6308\n",
      "Epoch 42/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4553 - pseudo_Y_loss: 0.8942 - S_pred_loss: 0.6959 - Y_pred_loss: 0.8652 - pseudo_Y_accuracy: 0.6015 - S_pred_accuracy: 0.5105 - Y_pred_accuracy: 0.6277\n",
      "Epoch 43/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.4289 - pseudo_Y_loss: 0.8839 - S_pred_loss: 0.6955 - Y_pred_loss: 0.8495 - pseudo_Y_accuracy: 0.6065 - S_pred_accuracy: 0.5102 - Y_pred_accuracy: 0.6342\n",
      "Epoch 44/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.4111 - pseudo_Y_loss: 0.8761 - S_pred_loss: 0.6949 - Y_pred_loss: 0.8400 - pseudo_Y_accuracy: 0.6185 - S_pred_accuracy: 0.5128 - Y_pred_accuracy: 0.6380\n",
      "Epoch 45/64\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.3959 - pseudo_Y_loss: 0.8701 - S_pred_loss: 0.6959 - Y_pred_loss: 0.8300 - pseudo_Y_accuracy: 0.6102 - S_pred_accuracy: 0.5082 - Y_pred_accuracy: 0.6442\n",
      "Epoch 46/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.3931 - pseudo_Y_loss: 0.8689 - S_pred_loss: 0.6963 - Y_pred_loss: 0.8279 - pseudo_Y_accuracy: 0.6170 - S_pred_accuracy: 0.5095 - Y_pred_accuracy: 0.6510\n",
      "Epoch 47/64\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.3771 - pseudo_Y_loss: 0.8628 - S_pred_loss: 0.6950 - Y_pred_loss: 0.8193 - pseudo_Y_accuracy: 0.6212 - S_pred_accuracy: 0.5182 - Y_pred_accuracy: 0.6505\n",
      "Epoch 48/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.3527 - pseudo_Y_loss: 0.8506 - S_pred_loss: 0.6945 - Y_pred_loss: 0.8076 - pseudo_Y_accuracy: 0.6230 - S_pred_accuracy: 0.5132 - Y_pred_accuracy: 0.6540\n",
      "Epoch 49/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.3480 - pseudo_Y_loss: 0.8490 - S_pred_loss: 0.6947 - Y_pred_loss: 0.8043 - pseudo_Y_accuracy: 0.6242 - S_pred_accuracy: 0.5113 - Y_pred_accuracy: 0.6605\n",
      "Epoch 50/64\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.3241 - pseudo_Y_loss: 0.8370 - S_pred_loss: 0.6939 - Y_pred_loss: 0.7931 - pseudo_Y_accuracy: 0.6388 - S_pred_accuracy: 0.5192 - Y_pred_accuracy: 0.6520\n",
      "Epoch 51/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.3139 - pseudo_Y_loss: 0.8353 - S_pred_loss: 0.6932 - Y_pred_loss: 0.7854 - pseudo_Y_accuracy: 0.6348 - S_pred_accuracy: 0.5120 - Y_pred_accuracy: 0.6658\n",
      "Epoch 52/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.3037 - pseudo_Y_loss: 0.8300 - S_pred_loss: 0.6940 - Y_pred_loss: 0.7796 - pseudo_Y_accuracy: 0.6392 - S_pred_accuracy: 0.5130 - Y_pred_accuracy: 0.6687\n",
      "Epoch 53/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.2809 - pseudo_Y_loss: 0.8194 - S_pred_loss: 0.6937 - Y_pred_loss: 0.7678 - pseudo_Y_accuracy: 0.6503 - S_pred_accuracy: 0.5138 - Y_pred_accuracy: 0.6750\n",
      "Epoch 54/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.2769 - pseudo_Y_loss: 0.8191 - S_pred_loss: 0.6937 - Y_pred_loss: 0.7641 - pseudo_Y_accuracy: 0.6410 - S_pred_accuracy: 0.5095 - Y_pred_accuracy: 0.6812\n",
      "Epoch 55/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.2628 - pseudo_Y_loss: 0.8118 - S_pred_loss: 0.6924 - Y_pred_loss: 0.7587 - pseudo_Y_accuracy: 0.6485 - S_pred_accuracy: 0.5200 - Y_pred_accuracy: 0.6745\n",
      "Epoch 56/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.2560 - pseudo_Y_loss: 0.8070 - S_pred_loss: 0.6938 - Y_pred_loss: 0.7552 - pseudo_Y_accuracy: 0.6497 - S_pred_accuracy: 0.5163 - Y_pred_accuracy: 0.6787\n",
      "Epoch 57/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.2456 - pseudo_Y_loss: 0.8056 - S_pred_loss: 0.6928 - Y_pred_loss: 0.7472 - pseudo_Y_accuracy: 0.6515 - S_pred_accuracy: 0.5155 - Y_pred_accuracy: 0.6837\n",
      "Epoch 58/64\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 2.2375 - pseudo_Y_loss: 0.8012 - S_pred_loss: 0.6919 - Y_pred_loss: 0.7445 - pseudo_Y_accuracy: 0.6532 - S_pred_accuracy: 0.5155 - Y_pred_accuracy: 0.6815\n",
      "Epoch 59/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.2190 - pseudo_Y_loss: 0.7926 - S_pred_loss: 0.6927 - Y_pred_loss: 0.7337 - pseudo_Y_accuracy: 0.6607 - S_pred_accuracy: 0.5135 - Y_pred_accuracy: 0.6890\n",
      "Epoch 60/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.2190 - pseudo_Y_loss: 0.7911 - S_pred_loss: 0.6930 - Y_pred_loss: 0.7350 - pseudo_Y_accuracy: 0.6600 - S_pred_accuracy: 0.5130 - Y_pred_accuracy: 0.6883\n",
      "Epoch 61/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.1981 - pseudo_Y_loss: 0.7822 - S_pred_loss: 0.6927 - Y_pred_loss: 0.7232 - pseudo_Y_accuracy: 0.6702 - S_pred_accuracy: 0.5160 - Y_pred_accuracy: 0.6913\n",
      "Epoch 62/64\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.2020 - pseudo_Y_loss: 0.7834 - S_pred_loss: 0.6941 - Y_pred_loss: 0.7246 - pseudo_Y_accuracy: 0.6615 - S_pred_accuracy: 0.5085 - Y_pred_accuracy: 0.6920\n",
      "Epoch 63/64\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.1919 - pseudo_Y_loss: 0.7786 - S_pred_loss: 0.6934 - Y_pred_loss: 0.7199 - pseudo_Y_accuracy: 0.6645 - S_pred_accuracy: 0.5092 - Y_pred_accuracy: 0.6960\n",
      "Epoch 64/64\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 2.1845 - pseudo_Y_loss: 0.7771 - S_pred_loss: 0.6927 - Y_pred_loss: 0.7147 - pseudo_Y_accuracy: 0.6693 - S_pred_accuracy: 0.5190 - Y_pred_accuracy: 0.6980\n",
      "125/125 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "\n",
      "Pseudo-label statistics (training):\n",
      "Group 0 pseudo-class distribution: [722 314 417 522]\n",
      "Group 1 pseudo-class distribution: [660 332 420 613]\n",
      "\n",
      "Training baseline [BIASED] logistic regression classifier (X → Y)...\n",
      "\n",
      "Training fair logistic regression classifier (X → Y') using pseudo-labels...\n",
      "\n",
      "Baseline Logistic Regression (X → Y) Evaluation:\n",
      "AUC: 0.9612, Accuracy: 0.8140\n",
      "Fairness metrics: {'demographic_parity_difference': 0.07288220551378444, 'equalized_odds_difference': 0.17304856432400867, 'selection_rate': {0: [0.3352380952380952, 0.2342857142857143, 0.24761904761904763, 0.18285714285714286], 1: [0.18947368421052632, 0.2694736842105263, 0.25473684210526315, 0.2863157894736842]}, 'group_accuracy': {0: 0.8038095238095239, 1: 0.8252631578947368}}\n",
      "\n",
      "Fair Logistic Regression (X → Y') Evaluation (compared to observed Y):\n",
      "AUC: 0.9612, Accuracy: 0.7630\n",
      "Fairness metrics: {'demographic_parity_difference': 0.051328320802005, 'equalized_odds_difference': 0.14048621472852085, 'selection_rate': {0: [0.4361904761904762, 0.11428571428571428, 0.22857142857142856, 0.22095238095238096], 1: [0.3431578947368421, 0.13894736842105262, 0.21894736842105264, 0.29894736842105263]}, 'group_accuracy': {0: 0.7447619047619047, 1: 0.783157894736842}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_synthetic(lambda_adv=1.0, epochs=64, batch_size=128) # working the best now!\n",
    "# main_synthetic(lambda_adv=1.0, epochs=64, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86e898-d71b-42d7-a0d9-ca66501381ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### last thing to do -- need to hypertune \n",
    "### need to do synthetic dataset for a multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f65247ba-a7aa-4e3b-a8fb-a3ef2b7905e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing lambda_adv=1.0, epochs=32, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([527, 180, 245, 382]))\n",
      "  Fold 1: Score=1.7393, Accuracy=0.8223, AUC=0.9648, Demographic Parity Diff=0.0479\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([413, 249, 222, 449]))\n",
      "  Fold 2: Score=1.6886, Accuracy=0.7989, AUC=0.9552, Demographic Parity Diff=0.0656\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([483, 174, 162, 514]))\n",
      "  Fold 3: Score=1.7159, Accuracy=0.8005, AUC=0.9561, Demographic Parity Diff=0.0406\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=32, batch_size=64: Score=1.7146, Accuracy=0.8072, AUC=0.9587, Demographic Parity Diff=0.0514\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=32, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([328, 239, 332, 435]))\n",
      "  Fold 1: Score=1.6947, Accuracy=0.8006, AUC=0.9587, Demographic Parity Diff=0.0647\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([317, 374, 452, 190]))\n",
      "  Fold 2: Score=1.6863, Accuracy=0.7997, AUC=0.9595, Demographic Parity Diff=0.0728\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([403, 357, 302, 271]))\n",
      "  Fold 3: Score=1.7128, Accuracy=0.8020, AUC=0.9575, Demographic Parity Diff=0.0467\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=32, batch_size=128: Score=1.6979, Accuracy=0.8008, AUC=0.9586, Demographic Parity Diff=0.0614\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=32, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([242, 270, 337, 485]))\n",
      "  Fold 1: Score=1.7309, Accuracy=0.8186, AUC=0.9636, Demographic Parity Diff=0.0513\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([351, 279, 302, 401]))\n",
      "  Fold 2: Score=1.6934, Accuracy=0.7884, AUC=0.9510, Demographic Parity Diff=0.0461\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([250, 334, 379, 370]))\n",
      "  Fold 3: Score=1.7334, Accuracy=0.8282, AUC=0.9650, Demographic Parity Diff=0.0599\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=32, batch_size=256: Score=1.7192, Accuracy=0.8117, AUC=0.9599, Demographic Parity Diff=0.0524\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=64, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([393, 203, 315, 423]))\n",
      "  Fold 1: Score=1.7066, Accuracy=0.8043, AUC=0.9592, Demographic Parity Diff=0.0570\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([401, 273, 298, 361]))\n",
      "  Fold 2: Score=1.7206, Accuracy=0.8230, AUC=0.9658, Demographic Parity Diff=0.0681\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([361, 278, 345, 349]))\n",
      "  Fold 3: Score=1.7217, Accuracy=0.8020, AUC=0.9549, Demographic Parity Diff=0.0351\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=64, batch_size=64: Score=1.7163, Accuracy=0.8098, AUC=0.9600, Demographic Parity Diff=0.0534\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=64, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([445, 176, 210, 503]))\n",
      "  Fold 1: Score=1.7088, Accuracy=0.8036, AUC=0.9605, Demographic Parity Diff=0.0553\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([355, 256, 276, 446]))\n",
      "  Fold 2: Score=1.7313, Accuracy=0.8230, AUC=0.9643, Demographic Parity Diff=0.0559\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([440, 228, 322, 343]))\n",
      "  Fold 3: Score=1.7078, Accuracy=0.8102, AUC=0.9600, Demographic Parity Diff=0.0623\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=64, batch_size=128: Score=1.7160, Accuracy=0.8123, AUC=0.9616, Demographic Parity Diff=0.0579\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=64, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([428, 225, 160, 521]))\n",
      "  Fold 1: Score=1.7391, Accuracy=0.8178, AUC=0.9636, Demographic Parity Diff=0.0423\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([493, 162, 370, 308]))\n",
      "  Fold 2: Score=1.6720, Accuracy=0.7839, AUC=0.9522, Demographic Parity Diff=0.0641\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([447, 227, 264, 395]))\n",
      "  Fold 3: Score=1.7185, Accuracy=0.8155, AUC=0.9617, Demographic Parity Diff=0.0586\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=64, batch_size=256: Score=1.7099, Accuracy=0.8057, AUC=0.9592, Demographic Parity Diff=0.0550\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=128, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([367, 246, 371, 350]))\n",
      "  Fold 1: Score=1.7112, Accuracy=0.8066, AUC=0.9579, Demographic Parity Diff=0.0533\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([334, 288, 384, 327]))\n",
      "  Fold 2: Score=1.6812, Accuracy=0.7847, AUC=0.9527, Demographic Parity Diff=0.0562\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([345, 301, 332, 355]))\n",
      "  Fold 3: Score=1.7496, Accuracy=0.8350, AUC=0.9643, Demographic Parity Diff=0.0497\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=128, batch_size=64: Score=1.7140, Accuracy=0.8088, AUC=0.9583, Demographic Parity Diff=0.0531\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=128, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([357, 305, 289, 383]))\n",
      "  Fold 1: Score=1.6998, Accuracy=0.7946, AUC=0.9574, Demographic Parity Diff=0.0522\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([407, 273, 312, 341]))\n",
      "  Fold 2: Score=1.6852, Accuracy=0.7952, AUC=0.9564, Demographic Parity Diff=0.0664\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([370, 275, 285, 403]))\n",
      "  Fold 3: Score=1.7325, Accuracy=0.8200, AUC=0.9623, Demographic Parity Diff=0.0497\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=128, batch_size=128: Score=1.7058, Accuracy=0.8033, AUC=0.9587, Demographic Parity Diff=0.0561\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=128, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([403, 283, 243, 405]))\n",
      "  Fold 1: Score=1.6654, Accuracy=0.7924, AUC=0.9539, Demographic Parity Diff=0.0809\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([411, 268, 181, 473]))\n",
      "  Fold 2: Score=1.6926, Accuracy=0.7997, AUC=0.9570, Demographic Parity Diff=0.0642\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([416, 258, 236, 423]))\n",
      "  Fold 3: Score=1.7776, Accuracy=0.8447, AUC=0.9693, Demographic Parity Diff=0.0364\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=128, batch_size=256: Score=1.7119, Accuracy=0.8123, AUC=0.9601, Demographic Parity Diff=0.0605\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=32, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([335, 265, 277, 457]))\n",
      "  Fold 1: Score=1.7152, Accuracy=0.8043, AUC=0.9618, Demographic Parity Diff=0.0509\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([334, 439, 320, 240]))\n",
      "  Fold 2: Score=1.7005, Accuracy=0.8057, AUC=0.9581, Demographic Parity Diff=0.0633\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([262, 402, 266, 403]))\n",
      "  Fold 3: Score=1.7246, Accuracy=0.8140, AUC=0.9598, Demographic Parity Diff=0.0491\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=32, batch_size=64: Score=1.7134, Accuracy=0.8080, AUC=0.9599, Demographic Parity Diff=0.0544\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=32, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([322, 289, 416, 307]))\n",
      "  Fold 1: Score=1.7574, Accuracy=0.8298, AUC=0.9673, Demographic Parity Diff=0.0397\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([439, 223, 263, 408]))\n",
      "  Fold 2: Score=1.7270, Accuracy=0.8140, AUC=0.9590, Demographic Parity Diff=0.0460\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([440, 257, 263, 373]))\n",
      "  Fold 3: Score=1.6627, Accuracy=0.7847, AUC=0.9512, Demographic Parity Diff=0.0733\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=32, batch_size=128: Score=1.7157, Accuracy=0.8095, AUC=0.9592, Demographic Parity Diff=0.0530\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=32, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([396, 377, 309, 252]))\n",
      "  Fold 1: Score=1.7219, Accuracy=0.8231, AUC=0.9613, Demographic Parity Diff=0.0625\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([252, 119, 554, 408]))\n",
      "  Fold 2: Score=1.7235, Accuracy=0.8192, AUC=0.9630, Demographic Parity Diff=0.0587\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([354, 345, 375, 259]))\n",
      "  Fold 3: Score=1.7047, Accuracy=0.8005, AUC=0.9579, Demographic Parity Diff=0.0537\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=32, batch_size=256: Score=1.7167, Accuracy=0.8142, AUC=0.9608, Demographic Parity Diff=0.0583\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=64, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([436, 191, 266, 441]))\n",
      "  Fold 1: Score=1.6979, Accuracy=0.8036, AUC=0.9567, Demographic Parity Diff=0.0624\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([368, 258, 307, 400]))\n",
      "  Fold 2: Score=1.6924, Accuracy=0.8012, AUC=0.9566, Demographic Parity Diff=0.0655\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([492, 328, 202, 311]))\n",
      "  Fold 3: Score=1.7454, Accuracy=0.8230, AUC=0.9666, Demographic Parity Diff=0.0442\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=64, batch_size=64: Score=1.7119, Accuracy=0.8093, AUC=0.9600, Demographic Parity Diff=0.0574\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=64, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([405, 208, 407, 314]))\n",
      "  Fold 1: Score=1.7231, Accuracy=0.8148, AUC=0.9662, Demographic Parity Diff=0.0580\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([380, 327, 321, 305]))\n",
      "  Fold 2: Score=1.6809, Accuracy=0.7929, AUC=0.9536, Demographic Parity Diff=0.0657\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([356, 194, 343, 440]))\n",
      "  Fold 3: Score=1.7060, Accuracy=0.8057, AUC=0.9581, Demographic Parity Diff=0.0578\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=64, batch_size=128: Score=1.7033, Accuracy=0.8045, AUC=0.9593, Demographic Parity Diff=0.0605\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=64, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([394, 464, 267, 209]))\n",
      "  Fold 1: Score=1.7311, Accuracy=0.8208, AUC=0.9650, Demographic Parity Diff=0.0547\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([470, 344, 322, 197]))\n",
      "  Fold 2: Score=1.7002, Accuracy=0.8035, AUC=0.9570, Demographic Parity Diff=0.0603\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([354, 215, 493, 271]))\n",
      "  Fold 3: Score=1.7298, Accuracy=0.8207, AUC=0.9615, Demographic Parity Diff=0.0524\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=64, batch_size=256: Score=1.7204, Accuracy=0.8150, AUC=0.9612, Demographic Parity Diff=0.0558\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=128, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([389, 212, 337, 396]))\n",
      "  Fold 1: Score=1.7147, Accuracy=0.7999, AUC=0.9568, Demographic Parity Diff=0.0419\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([375, 291, 291, 376]))\n",
      "  Fold 2: Score=1.7000, Accuracy=0.8087, AUC=0.9633, Demographic Parity Diff=0.0719\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([387, 324, 212, 410]))\n",
      "  Fold 3: Score=1.7124, Accuracy=0.8072, AUC=0.9583, Demographic Parity Diff=0.0530\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=128, batch_size=64: Score=1.7090, Accuracy=0.8053, AUC=0.9594, Demographic Parity Diff=0.0556\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=128, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([448, 170, 276, 440]))\n",
      "  Fold 1: Score=1.6581, Accuracy=0.7796, AUC=0.9538, Demographic Parity Diff=0.0753\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([396, 283, 185, 469]))\n",
      "  Fold 2: Score=1.7205, Accuracy=0.8095, AUC=0.9600, Demographic Parity Diff=0.0490\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([424, 203, 236, 470]))\n",
      "  Fold 3: Score=1.7597, Accuracy=0.8335, AUC=0.9684, Demographic Parity Diff=0.0421\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=128, batch_size=128: Score=1.7128, Accuracy=0.8075, AUC=0.9607, Demographic Parity Diff=0.0555\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=128, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([482, 237, 226, 389]))\n",
      "  Fold 1: Score=1.6832, Accuracy=0.7924, AUC=0.9545, Demographic Parity Diff=0.0637\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([325, 278, 404, 326]))\n",
      "  Fold 2: Score=1.6644, Accuracy=0.7922, AUC=0.9550, Demographic Parity Diff=0.0828\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([354, 377, 134, 468]))\n",
      "  Fold 3: Score=1.7700, Accuracy=0.8357, AUC=0.9674, Demographic Parity Diff=0.0330\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=128, batch_size=256: Score=1.7059, Accuracy=0.8068, AUC=0.9590, Demographic Parity Diff=0.0598\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=32, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([351, 379, 256, 348]))\n",
      "  Fold 1: Score=1.7045, Accuracy=0.8058, AUC=0.9625, Demographic Parity Diff=0.0639\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([395, 353, 281, 304]))\n",
      "  Fold 2: Score=1.6959, Accuracy=0.8027, AUC=0.9581, Demographic Parity Diff=0.0649\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([257, 317, 481, 278]))\n",
      "  Fold 3: Score=1.7278, Accuracy=0.8140, AUC=0.9623, Demographic Parity Diff=0.0484\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=32, batch_size=64: Score=1.7094, Accuracy=0.8075, AUC=0.9610, Demographic Parity Diff=0.0591\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=32, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([417, 344, 378, 195]))\n",
      "  Fold 1: Score=1.7465, Accuracy=0.8223, AUC=0.9644, Demographic Parity Diff=0.0402\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([300, 350, 269, 414]))\n",
      "  Fold 2: Score=1.7077, Accuracy=0.8162, AUC=0.9624, Demographic Parity Diff=0.0709\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([462, 269, 283, 319]))\n",
      "  Fold 3: Score=1.6932, Accuracy=0.7959, AUC=0.9556, Demographic Parity Diff=0.0584\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=32, batch_size=128: Score=1.7158, Accuracy=0.8115, AUC=0.9608, Demographic Parity Diff=0.0565\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=32, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([325, 458, 194, 357]))\n",
      "  Fold 1: Score=1.7545, Accuracy=0.8283, AUC=0.9681, Demographic Parity Diff=0.0420\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([322, 348, 393, 270]))\n",
      "  Fold 2: Score=1.6714, Accuracy=0.7817, AUC=0.9531, Demographic Parity Diff=0.0634\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([355, 329, 268, 381]))\n",
      "  Fold 3: Score=1.7010, Accuracy=0.8125, AUC=0.9627, Demographic Parity Diff=0.0742\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=32, batch_size=256: Score=1.7090, Accuracy=0.8075, AUC=0.9613, Demographic Parity Diff=0.0599\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=64, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([394, 244, 403, 293]))\n",
      "  Fold 1: Score=1.7453, Accuracy=0.8268, AUC=0.9633, Demographic Parity Diff=0.0448\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([294, 173, 542, 324]))\n",
      "  Fold 2: Score=1.7335, Accuracy=0.8200, AUC=0.9626, Demographic Parity Diff=0.0490\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([318, 324, 326, 365]))\n",
      "  Fold 3: Score=1.6817, Accuracy=0.7944, AUC=0.9549, Demographic Parity Diff=0.0677\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=64, batch_size=64: Score=1.7202, Accuracy=0.8137, AUC=0.9602, Demographic Parity Diff=0.0538\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=64, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([350, 261, 336, 387]))\n",
      "  Fold 1: Score=1.6796, Accuracy=0.7879, AUC=0.9518, Demographic Parity Diff=0.0601\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([390, 301, 338, 304]))\n",
      "  Fold 2: Score=1.7266, Accuracy=0.8125, AUC=0.9581, Demographic Parity Diff=0.0440\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([331, 272, 398, 332]))\n",
      "  Fold 3: Score=1.7249, Accuracy=0.8170, AUC=0.9640, Demographic Parity Diff=0.0560\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=64, batch_size=128: Score=1.7104, Accuracy=0.8058, AUC=0.9580, Demographic Parity Diff=0.0534\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=64, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([270, 376, 245, 443]))\n",
      "  Fold 1: Score=1.7287, Accuracy=0.8133, AUC=0.9597, Demographic Parity Diff=0.0443\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([351, 474, 302, 206]))\n",
      "  Fold 2: Score=1.7277, Accuracy=0.8095, AUC=0.9600, Demographic Parity Diff=0.0417\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([273, 350, 381, 329]))\n",
      "  Fold 3: Score=1.7316, Accuracy=0.8260, AUC=0.9646, Demographic Parity Diff=0.0589\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=64, batch_size=256: Score=1.7294, Accuracy=0.8163, AUC=0.9614, Demographic Parity Diff=0.0483\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=128, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([429, 265, 262, 378]))\n",
      "  Fold 1: Score=1.7268, Accuracy=0.8156, AUC=0.9630, Demographic Parity Diff=0.0518\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([395, 188, 236, 514]))\n",
      "  Fold 2: Score=1.7026, Accuracy=0.7967, AUC=0.9517, Demographic Parity Diff=0.0458\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([436, 251, 214, 432]))\n",
      "  Fold 3: Score=1.7255, Accuracy=0.8192, AUC=0.9633, Demographic Parity Diff=0.0570\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=128, batch_size=64: Score=1.7183, Accuracy=0.8105, AUC=0.9593, Demographic Parity Diff=0.0516\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=128, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([206, 408, 369, 351]))\n",
      "  Fold 1: Score=1.6833, Accuracy=0.7954, AUC=0.9541, Demographic Parity Diff=0.0661\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([311, 388, 265, 369]))\n",
      "  Fold 2: Score=1.7203, Accuracy=0.8087, AUC=0.9609, Demographic Parity Diff=0.0492\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([357, 271, 341, 364]))\n",
      "  Fold 3: Score=1.7381, Accuracy=0.8290, AUC=0.9652, Demographic Parity Diff=0.0560\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=128, batch_size=128: Score=1.7139, Accuracy=0.8110, AUC=0.9600, Demographic Parity Diff=0.0571\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=128, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([300, 397, 274, 363]))\n",
      "  Fold 1: Score=1.6750, Accuracy=0.7894, AUC=0.9558, Demographic Parity Diff=0.0701\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([325, 373, 198, 437]))\n",
      "  Fold 2: Score=1.7482, Accuracy=0.8185, AUC=0.9584, Demographic Parity Diff=0.0287\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([469, 108, 395, 361]))\n",
      "  Fold 3: Score=1.7120, Accuracy=0.8087, AUC=0.9627, Demographic Parity Diff=0.0594\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=128, batch_size=256: Score=1.7117, Accuracy=0.8055, AUC=0.9590, Demographic Parity Diff=0.0527\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=32, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([353, 337, 385, 259]))\n",
      "  Fold 1: Score=1.7117, Accuracy=0.8111, AUC=0.9603, Demographic Parity Diff=0.0598\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([234, 537, 265, 297]))\n",
      "  Fold 2: Score=1.7215, Accuracy=0.8087, AUC=0.9631, Demographic Parity Diff=0.0503\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([385, 344, 185, 419]))\n",
      "  Fold 3: Score=1.6974, Accuracy=0.8057, AUC=0.9590, Demographic Parity Diff=0.0673\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=32, batch_size=64: Score=1.7102, Accuracy=0.8085, AUC=0.9608, Demographic Parity Diff=0.0591\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=32, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([333, 370, 313, 318]))\n",
      "  Fold 1: Score=1.7456, Accuracy=0.8163, AUC=0.9646, Demographic Parity Diff=0.0353\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([457, 353, 239, 284]))\n",
      "  Fold 2: Score=1.6939, Accuracy=0.8117, AUC=0.9579, Demographic Parity Diff=0.0757\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([486, 160, 421, 266]))\n",
      "  Fold 3: Score=1.7157, Accuracy=0.8125, AUC=0.9609, Demographic Parity Diff=0.0577\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=32, batch_size=128: Score=1.7184, Accuracy=0.8135, AUC=0.9611, Demographic Parity Diff=0.0562\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=32, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([337, 367, 335, 295]))\n",
      "  Fold 1: Score=1.7083, Accuracy=0.8036, AUC=0.9576, Demographic Parity Diff=0.0528\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([315, 524, 140, 354]))\n",
      "  Fold 2: Score=1.7038, Accuracy=0.8027, AUC=0.9569, Demographic Parity Diff=0.0558\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([381, 302, 296, 354]))\n",
      "  Fold 3: Score=1.7369, Accuracy=0.8275, AUC=0.9646, Demographic Parity Diff=0.0552\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=32, batch_size=256: Score=1.7163, Accuracy=0.8113, AUC=0.9597, Demographic Parity Diff=0.0546\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=64, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([393, 415, 214, 312]))\n",
      "  Fold 1: Score=1.7222, Accuracy=0.8096, AUC=0.9597, Demographic Parity Diff=0.0471\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([324, 307, 319, 383]))\n",
      "  Fold 2: Score=1.7430, Accuracy=0.8282, AUC=0.9650, Demographic Parity Diff=0.0502\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([169, 281, 384, 499]))\n",
      "  Fold 3: Score=1.7025, Accuracy=0.7974, AUC=0.9577, Demographic Parity Diff=0.0526\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=64, batch_size=64: Score=1.7226, Accuracy=0.8118, AUC=0.9608, Demographic Parity Diff=0.0500\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=64, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([436, 357, 296, 245]))\n",
      "  Fold 1: Score=1.7024, Accuracy=0.8021, AUC=0.9581, Demographic Parity Diff=0.0578\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([281, 520, 172, 360]))\n",
      "  Fold 2: Score=1.7480, Accuracy=0.8305, AUC=0.9691, Demographic Parity Diff=0.0515\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([306, 344, 339, 344]))\n",
      "  Fold 3: Score=1.6968, Accuracy=0.7922, AUC=0.9551, Demographic Parity Diff=0.0505\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=64, batch_size=128: Score=1.7158, Accuracy=0.8083, AUC=0.9608, Demographic Parity Diff=0.0532\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=64, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([129, 438, 398, 369]))\n",
      "  Fold 1: Score=1.7086, Accuracy=0.8006, AUC=0.9541, Demographic Parity Diff=0.0460\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([480, 248, 293, 312]))\n",
      "  Fold 2: Score=1.7130, Accuracy=0.8072, AUC=0.9585, Demographic Parity Diff=0.0527\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([161, 230, 271, 671]))\n",
      "  Fold 3: Score=1.7064, Accuracy=0.8192, AUC=0.9680, Demographic Parity Diff=0.0807\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=64, batch_size=256: Score=1.7094, Accuracy=0.8090, AUC=0.9602, Demographic Parity Diff=0.0598\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=128, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([300, 232, 347, 455]))\n",
      "  Fold 1: Score=1.7175, Accuracy=0.7999, AUC=0.9588, Demographic Parity Diff=0.0412\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([504, 300, 200, 329]))\n",
      "  Fold 2: Score=1.7425, Accuracy=0.8215, AUC=0.9632, Demographic Parity Diff=0.0422\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([292, 345, 324, 372]))\n",
      "  Fold 3: Score=1.6868, Accuracy=0.8110, AUC=0.9619, Demographic Parity Diff=0.0861\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=128, batch_size=64: Score=1.7156, Accuracy=0.8108, AUC=0.9613, Demographic Parity Diff=0.0565\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=128, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([307, 232, 324, 471]))\n",
      "  Fold 1: Score=1.7443, Accuracy=0.8268, AUC=0.9654, Demographic Parity Diff=0.0480\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([364, 249, 247, 473]))\n",
      "  Fold 2: Score=1.6927, Accuracy=0.8102, AUC=0.9625, Demographic Parity Diff=0.0800\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([270, 409, 250, 404]))\n",
      "  Fold 3: Score=1.6843, Accuracy=0.7892, AUC=0.9526, Demographic Parity Diff=0.0574\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=128, batch_size=128: Score=1.7071, Accuracy=0.8087, AUC=0.9602, Demographic Parity Diff=0.0618\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=128, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([368, 343, 268, 355]))\n",
      "  Fold 1: Score=1.6955, Accuracy=0.8043, AUC=0.9599, Demographic Parity Diff=0.0688\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([518, 315, 290, 210]))\n",
      "  Fold 2: Score=1.6766, Accuracy=0.7832, AUC=0.9516, Demographic Parity Diff=0.0582\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([368, 314, 275, 376]))\n",
      "  Fold 3: Score=1.7548, Accuracy=0.8267, AUC=0.9642, Demographic Parity Diff=0.0361\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=128, batch_size=256: Score=1.7090, Accuracy=0.8048, AUC=0.9586, Demographic Parity Diff=0.0544\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=32, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([161, 470, 376, 327]))\n",
      "  Fold 1: Score=1.7002, Accuracy=0.7939, AUC=0.9545, Demographic Parity Diff=0.0481\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([379, 310, 281, 363]))\n",
      "  Fold 2: Score=1.7367, Accuracy=0.8290, AUC=0.9673, Demographic Parity Diff=0.0595\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([285, 467, 221, 360]))\n",
      "  Fold 3: Score=1.6881, Accuracy=0.8065, AUC=0.9627, Demographic Parity Diff=0.0811\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=32, batch_size=64: Score=1.7083, Accuracy=0.8098, AUC=0.9615, Demographic Parity Diff=0.0629\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=32, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([414, 190, 414, 316]))\n",
      "  Fold 1: Score=1.7143, Accuracy=0.8073, AUC=0.9580, Demographic Parity Diff=0.0511\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([504, 178, 400, 251]))\n",
      "  Fold 2: Score=1.7233, Accuracy=0.8147, AUC=0.9639, Demographic Parity Diff=0.0553\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([211, 331, 404, 387]))\n",
      "  Fold 3: Score=1.6973, Accuracy=0.8035, AUC=0.9542, Demographic Parity Diff=0.0604\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=32, batch_size=128: Score=1.7116, Accuracy=0.8085, AUC=0.9587, Demographic Parity Diff=0.0556\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=32, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([241, 234, 295, 564]))\n",
      "  Fold 1: Score=1.7593, Accuracy=0.8298, AUC=0.9666, Demographic Parity Diff=0.0371\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([315, 231, 355, 432]))\n",
      "  Fold 2: Score=1.6923, Accuracy=0.8057, AUC=0.9596, Demographic Parity Diff=0.0730\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([367, 313, 318, 335]))\n",
      "  Fold 3: Score=1.6928, Accuracy=0.7959, AUC=0.9558, Demographic Parity Diff=0.0589\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=32, batch_size=256: Score=1.7148, Accuracy=0.8105, AUC=0.9607, Demographic Parity Diff=0.0563\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=64, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([503, 250, 329, 252]))\n",
      "  Fold 1: Score=1.7093, Accuracy=0.8118, AUC=0.9606, Demographic Parity Diff=0.0631\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([130, 196, 524, 483]))\n",
      "  Fold 2: Score=1.6960, Accuracy=0.8095, AUC=0.9559, Demographic Parity Diff=0.0694\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([518, 306, 240, 269]))\n",
      "  Fold 3: Score=1.7272, Accuracy=0.8162, AUC=0.9633, Demographic Parity Diff=0.0522\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=64, batch_size=64: Score=1.7109, Accuracy=0.8125, AUC=0.9599, Demographic Parity Diff=0.0616\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=64, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([394, 283, 332, 325]))\n",
      "  Fold 1: Score=1.6987, Accuracy=0.8036, AUC=0.9593, Demographic Parity Diff=0.0642\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([229, 359, 234, 511]))\n",
      "  Fold 2: Score=1.7030, Accuracy=0.8005, AUC=0.9587, Demographic Parity Diff=0.0561\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([254, 255, 216, 608]))\n",
      "  Fold 3: Score=1.7079, Accuracy=0.8080, AUC=0.9607, Demographic Parity Diff=0.0607\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=64, batch_size=128: Score=1.7032, Accuracy=0.8040, AUC=0.9596, Demographic Parity Diff=0.0604\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=64, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([406, 290, 227, 411]))\n",
      "  Fold 1: Score=1.7440, Accuracy=0.8298, AUC=0.9664, Demographic Parity Diff=0.0522\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([298, 372, 306, 357]))\n",
      "  Fold 2: Score=1.7140, Accuracy=0.8005, AUC=0.9574, Demographic Parity Diff=0.0439\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([315, 301, 450, 267]))\n",
      "  Fold 3: Score=1.6679, Accuracy=0.7854, AUC=0.9537, Demographic Parity Diff=0.0713\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=64, batch_size=256: Score=1.7086, Accuracy=0.8052, AUC=0.9592, Demographic Parity Diff=0.0558\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=128, batch_size=64\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([278, 439, 290, 327]))\n",
      "  Fold 1: Score=1.6992, Accuracy=0.8021, AUC=0.9549, Demographic Parity Diff=0.0578\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([439, 398, 345, 151]))\n",
      "  Fold 2: Score=1.7122, Accuracy=0.8170, AUC=0.9638, Demographic Parity Diff=0.0686\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([222, 183, 550, 378]))\n",
      "  Fold 3: Score=1.7311, Accuracy=0.8132, AUC=0.9621, Demographic Parity Diff=0.0442\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=128, batch_size=64: Score=1.7141, Accuracy=0.8108, AUC=0.9603, Demographic Parity Diff=0.0569\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=128, batch_size=128\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([415, 362, 331, 226]))\n",
      "  Fold 1: Score=1.6939, Accuracy=0.8013, AUC=0.9600, Demographic Parity Diff=0.0674\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([345, 280, 315, 393]))\n",
      "  Fold 2: Score=1.6836, Accuracy=0.7982, AUC=0.9566, Demographic Parity Diff=0.0711\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([319, 502, 160, 352]))\n",
      "  Fold 3: Score=1.7657, Accuracy=0.8305, AUC=0.9641, Demographic Parity Diff=0.0289\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=128, batch_size=128: Score=1.7144, Accuracy=0.8100, AUC=0.9602, Demographic Parity Diff=0.0558\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=128, batch_size=256\n",
      "Fold 1 - Sample Predictions: (array([0, 1, 2, 3]), array([440, 315, 244, 335]))\n",
      "  Fold 1: Score=1.7070, Accuracy=0.8103, AUC=0.9606, Demographic Parity Diff=0.0639\n",
      "Fold 2 - Sample Predictions: (array([0, 1, 2, 3]), array([229, 460, 437, 207]))\n",
      "  Fold 2: Score=1.7250, Accuracy=0.8117, AUC=0.9630, Demographic Parity Diff=0.0498\n",
      "Fold 3 - Sample Predictions: (array([0, 1, 2, 3]), array([364, 410, 201, 358]))\n",
      "  Fold 3: Score=1.7127, Accuracy=0.8020, AUC=0.9553, Demographic Parity Diff=0.0445\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=128, batch_size=256: Score=1.7149, Accuracy=0.8080, AUC=0.9596, Demographic Parity Diff=0.0527\n",
      "\n",
      "Best Hyperparameters: lambda_adv                   5.000000\n",
      "epochs                      64.000000\n",
      "batch_size                 256.000000\n",
      "score                        1.729358\n",
      "accuracy                     0.816251\n",
      "auc                          0.961417\n",
      "demographic_parity_diff      0.048310\n",
      "Name: 23, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class AdversarialModelWrapperFixed(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Fixed Wrapper for Adversarial Model to work with Grid Search (Multi-Class).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambda_adv=1.0, epochs=64, batch_size=128):\n",
    "        self.lambda_adv = lambda_adv\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y, S):\n",
    "        \"\"\"\n",
    "        Train the adversarial model. S is passed dynamically per fold.\n",
    "        \"\"\"\n",
    "        y = to_categorical(y, num_classes=4)  # Convert `y` to one-hot encoding for multi-class\n",
    "        input_dim = X.shape[1]\n",
    "        S_oh = tf.keras.utils.to_categorical(S, num_classes=2)\n",
    "\n",
    "        # Ensure model is reinitialized for every run\n",
    "        tf.keras.backend.clear_session()\n",
    "        self.model = build_adversarial_model(input_dim, lambda_adv=self.lambda_adv)\n",
    "\n",
    "        self.model.fit(\n",
    "            [X, S_oh],\n",
    "            {\"pseudo_Y\": y, \"S_pred\": S_oh, \"Y_pred\": y},\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, S):\n",
    "        \"\"\"\n",
    "        Generate predictions from the trained model. S must match X per fold.\n",
    "        \"\"\"\n",
    "        S_oh = tf.keras.utils.to_categorical(S, num_classes=2)\n",
    "        pseudo_Y, _, _ = self.model.predict([X, S_oh], verbose=0)\n",
    "        return np.argmax(pseudo_Y, axis=1)  # Convert softmax probabilities to class predictions\n",
    "\n",
    "    def score(self, X_train, Y_train_biased_pred, X_test, Y_test_biased_pred, Y_train_raw, Y_test_raw, S_train, S_test, return_metrics=False):\n",
    "        \"\"\"\n",
    "        Compute the optimization score combining AUC, accuracy, and fairness metrics.\n",
    "        \"\"\"\n",
    "        auc, acc, fairness_metrics = run_biased_logistic(\n",
    "            X_train, Y_train_biased_pred, X_test, Y_test_biased_pred, \n",
    "            Y_train_raw, Y_test_raw, S_train, S_test\n",
    "        )\n",
    "        \n",
    "        demographic_parity_diff = abs(fairness_metrics[\"demographic_parity_difference\"])\n",
    "\n",
    "        # Objective function (equal weights for now)\n",
    "        score = auc + acc - demographic_parity_diff\n",
    "\n",
    "        if return_metrics:\n",
    "            return score, acc, auc, demographic_parity_diff\n",
    "        return score\n",
    "\n",
    "\n",
    "# Load synthetic dataset\n",
    "set_seed(42)\n",
    "X_train, X_test, Y_train_raw, Y_test_raw, S_train, S_test = generate_synthetic_data()\n",
    "Y_train_biased, Y_test_biased = inject_bias(bias_factor=0.3, seed=42)\n",
    "\n",
    "# Ensure `Y_train_biased` is a 1D array (convert categorical indices back)\n",
    "Y_train_biased = Y_train_biased.ravel()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"lambda_adv\": [1.0, 3.0, 5.0, 7.0, 15.0],\n",
    "    \"epochs\": [32, 64, 128],\n",
    "    \"batch_size\": [64, 128, 256]\n",
    "}\n",
    "\n",
    "# Custom cross-validation (Ensure stratification works correctly for multi-class)\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)  # Removing fixed random_state\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Perform manual grid search\n",
    "for lambda_adv, epochs, batch_size in product(param_grid[\"lambda_adv\"], param_grid[\"epochs\"], param_grid[\"batch_size\"]):\n",
    "    scores, accuracies, aucs, demographic_parity_diffs = [], [], [], []\n",
    "    \n",
    "    print(f\"\\nTesting lambda_adv={lambda_adv}, epochs={epochs}, batch_size={batch_size}\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, Y_train_biased)):  # Ensure Y_train_biased is used for stratification\n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        Y_train_fold, Y_val_fold = Y_train_biased[train_idx], Y_train_biased[val_idx]\n",
    "        S_train_fold, S_val_fold = S_train[train_idx], S_train[val_idx]\n",
    "\n",
    "        # Train model\n",
    "        model = AdversarialModelWrapperFixed(lambda_adv=lambda_adv, epochs=epochs, batch_size=batch_size)\n",
    "        model.fit(X_train_fold, Y_train_fold, S=S_train_fold)\n",
    "\n",
    "        # Evaluate model (Pass required arguments to `score`)\n",
    "        score, accuracy, auc, demographic_parity_diff = model.score(\n",
    "            X_train_fold, Y_train_fold, \n",
    "            X_val_fold, Y_val_fold, \n",
    "            Y_train_raw[train_idx], Y_train_raw[val_idx],  \n",
    "            S_train_fold, S_val_fold, \n",
    "            return_metrics=True\n",
    "        )\n",
    "\n",
    "        # Debug: Check if predictions are changing\n",
    "        preds = model.predict(X_val_fold, S_val_fold)\n",
    "        print(f\"Fold {fold + 1} - Sample Predictions: {np.unique(preds, return_counts=True)}\")  # Ensure different classes appear\n",
    "\n",
    "        # Store fold results\n",
    "        scores.append(score)\n",
    "        accuracies.append(accuracy)\n",
    "        aucs.append(auc)\n",
    "        demographic_parity_diffs.append(demographic_parity_diff)\n",
    "\n",
    "        # Print results per fold\n",
    "        print(f\"  Fold {fold + 1}: Score={score:.4f}, Accuracy={accuracy:.4f}, AUC={auc:.4f}, Demographic Parity Diff={demographic_parity_diff:.4f}\")\n",
    "\n",
    "    # Store average scores across folds\n",
    "    avg_score = np.mean(scores)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_auc = np.mean(aucs)\n",
    "    avg_demographic_parity_diff = np.mean(demographic_parity_diffs)\n",
    "\n",
    "    results.append({\n",
    "        \"lambda_adv\": lambda_adv,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"score\": avg_score,\n",
    "        \"accuracy\": avg_accuracy,\n",
    "        \"auc\": avg_auc,\n",
    "        \"demographic_parity_diff\": avg_demographic_parity_diff\n",
    "    })\n",
    "\n",
    "    print(f\"  Final (Avg) for lambda_adv={lambda_adv}, epochs={epochs}, batch_size={batch_size}: \"\n",
    "          f\"Score={avg_score:.4f}, Accuracy={avg_accuracy:.4f}, AUC={avg_auc:.4f}, Demographic Parity Diff={avg_demographic_parity_diff:.4f}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find best hyperparameters\n",
    "best_params = results_df.loc[results_df[\"score\"].idxmax()]\n",
    "print(\"\\nBest Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e4102-7834-4fa7-861c-33a025a58553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ebd500-9c2a-4fbc-b363-fd79165e2187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
