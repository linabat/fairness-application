{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf22c554-b495-4551-ae60-0381413fb23b",
   "metadata": {},
   "source": [
    "#### Plan for synthetic data \n",
    "1. Create synthetic datasets\n",
    "2. Run synthetic unbiased fair synthetic dataset\n",
    "3. add in error + bias into Y-train, make it rely on S\n",
    "4. run pipeline on new data and try to get it to meet the same baseline as the original dataset for demographic parity + accuracy.\n",
    "\n",
    "#### Additional work \n",
    "hypertune for lambda so that accuracy and demographic parity are well balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24672b93-3c9d-4e08-98b2-b6a106a323bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 09:52:49.760486: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-21 09:52:49.760527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-21 09:52:49.762116: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-21 09:52:49.770882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# !pip install fairlearn\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Flatten, \n",
    "    MaxPooling2D, BatchNormalization, Dropout\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Flatten, \n",
    "    MaxPooling2D, BatchNormalization, Dropout, Concatenate\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler\n",
    ")\n",
    "\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9af067-aa00-404e-9bbb-9fa55fd4367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Custom Gradient Reversal Layer\n",
    "# -------------------------------\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x, lambda_):\n",
    "    def grad(dy):\n",
    "        return -lambda_ * dy, None # reverses direction of gradient \n",
    "    return x, grad\n",
    "\n",
    "# custom Keras layer\n",
    "\"\"\"\n",
    "Layer is used to ensure that the feature representation are independent of a sensitive attribute\n",
    "- feature extract learns normally in the forward pass\n",
    "- reversing gradients of classifier that tries to predict the sensitive attribute during backpropagation -- stops feature extractor from encoding sensitive information\n",
    "\"\"\"\n",
    "class GradientReversalLayer(tf.keras.layers.Layer): \n",
    "    def __init__(self, lambda_=1.0, **kwargs):\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "        self.lambda_ = lambda_ # strength of gradient reversal\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x, self.lambda_)\n",
    "\n",
    "# -------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# -------------------------------\n",
    "def set_seed(seed_num):\n",
    "    random.seed(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    tf.random.set_seed(seed_num)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a279a6-a67d-4bab-8049-18a4eb1ab4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Adversarial Debiasing Model\n",
    "# -------------------------------\n",
    "def build_adversarial_model(input_dim, lambda_adv=1.0):\n",
    "    \"\"\"\n",
    "    Build an adversarial debiasing model that learns pseudo‑labels Y' from X.\n",
    "\n",
    "    Architecture:\n",
    "      - Main branch (encoder): from X, several dense layers produce a latent pseudo‑label pseudo_Y (via sigmoid).\n",
    "      - Adversary branch: pseudo_Y is passed through a Gradient Reversal Layer and then dense layers predict S.\n",
    "      - Decoder branch: concatenates pseudo_Y and the one-hot sensitive attribute S to predict the observed label Y.\n",
    "\n",
    "    Losses:\n",
    "      - For the main branch, binary crossentropy between observed Y and pseudo_Y (and Y_pred).\n",
    "      - For the adversary branch, categorical crossentropy to predict S.\n",
    "\n",
    "    Returns a compiled Keras model that takes inputs X and S (one-hot encoded) and outputs:\n",
    "      [pseudo_Y, S_pred, Y_pred].\n",
    "    \"\"\"\n",
    "    X_input = tf.keras.Input(shape=(input_dim,), name=\"X\")\n",
    "    S_input = tf.keras.Input(shape=(2,), name=\"S\")  # one-hot encoded S\n",
    "\n",
    "    # Main branch: Encoder for pseudo-label.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    h = Dense(64, activation='relu')(X_input)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Dense(32, activation='relu')(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    pseudo_Y = Dense(1, activation='sigmoid', name=\"pseudo_Y\")(h) ## outputs  probability value for pseudo_Y between 0,1\n",
    "\n",
    "    # Adversary branch: from pseudo_Y, with GRL.\n",
    "    \"\"\"\n",
    "    This is to prevent psuedo_Y from containing information about S\n",
    "    - adversary will try to predict S from pseudo_Y (fair label)...if it can accurately predict S, then Y' still encodes information about S (don't want this) \n",
    "    - use the gradient reversal layer to prevent this from happening\n",
    "    \"\"\"\n",
    "    grl = GradientReversalLayer(lambda_=lambda_adv)(pseudo_Y)\n",
    "    a = Dense(32, activation='relu')(grl)\n",
    "    a = BatchNormalization()(a)\n",
    "    S_pred = Dense(2, activation='softmax', name=\"S_pred\")(a)\n",
    "\n",
    "    # Decoder branch: combine pseudo_Y and S to predict observed Y.\n",
    "    \"\"\"\n",
    "    Y depepends on both Y' and S \n",
    "    -- predict the final observed label Y using both psuedo_Y and S\n",
    "    -- Y may still depend on S, that is why it's being used here \n",
    "    -- decoder ensures Y_final is accurate, while psuedo_Y is not directly influenced by S \n",
    "    -- psuedo_Y removes unfair dependencies on S...however S might still contain legit info needed to predict Y accurately \n",
    "    -- IMPORTANT - THIS STEP ALLOWS FAIR DEPENDENCIES WHILE ELIMINATING UNFAIR ONES\n",
    "    -- structure how S influences Y, without letting hidden biases leak through \n",
    "    \"\"\"\n",
    "    concat = Concatenate()([pseudo_Y, S_input])\n",
    "    d = Dense(16, activation='relu')(concat)\n",
    "    d = BatchNormalization()(d)\n",
    "    Y_pred = Dense(1, activation='sigmoid', name=\"Y_pred\")(d)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[X_input, S_input],\n",
    "                           outputs=[pseudo_Y, S_pred, Y_pred])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  loss={\"pseudo_Y\": \"binary_crossentropy\",\n",
    "                        \"S_pred\": \"categorical_crossentropy\",\n",
    "                        \"Y_pred\": \"binary_crossentropy\"},\n",
    "                  loss_weights={\"pseudo_Y\": 1.0, \"S_pred\": lambda_adv, \"Y_pred\": 1.0},\n",
    "                  metrics={\"pseudo_Y\": \"accuracy\",\n",
    "                           \"S_pred\": \"accuracy\",\n",
    "                           \"Y_pred\": \"accuracy\"}) # Y_pred is the best estimate of Y accounting for fair dependencies \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdce495e-4c4f-4024-81ae-ec51509beaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Manual Fairness Metrics\n",
    "# -------------------------------\n",
    "def compute_fairness_metrics_manual(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Compute fairness metrics manually.\n",
    "    y_true: binary ground-truth labels (1-D numpy array).\n",
    "    y_pred: continuous scores (will be thresholded at 0.5).\n",
    "    sensitive_features: 1-D numpy array (0 or 1).\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - Demographic parity difference (absolute difference in positive rates).\n",
    "      - Equalized odds difference (average difference in TPR and FPR).\n",
    "      - Selection rates per group.\n",
    "      - Group-wise accuracy.\n",
    "    \"\"\"\n",
    "    y_pred_bin = (y_pred > 0.5).astype(int) # y_pred is continuous value, so converting it to binary \n",
    "    groups = np.unique(sensitive_features)\n",
    "\n",
    "    # Demographic parity \n",
    "    \"\"\"\n",
    "    All groups (from sensitive feature) should receive positive predictions at the same rate\n",
    "    P(Y_hat = 1|S=0) = P(Y_hat=1|S=1)\n",
    "    \"\"\"\n",
    "\n",
    "    # For each group in the sensitive feature, find the demographic parity and compute the difference (based on the formula in above comment)\n",
    "    pos_rates = {}\n",
    "    for g in groups: \n",
    "        pos_rates[g] = np.mean(y_pred_bin[sensitive_features == g])\n",
    "    dp_diff = abs(pos_rates[0] - pos_rates[1]) ## this line assumes that there are only 2 groups, 0 and 1 -- if there are more than 2 groups, this would need to be changed\n",
    "    ## in all the examples used, there were only 2 groups -- need to double check this when working on new data\n",
    "    \n",
    "    # dp_diff > 0, then demographic parity isn't fair \n",
    "\n",
    "    # Equalized odds\n",
    "    \"\"\"\n",
    "    Ensuring the different groups in the sensitive feature similar TPR and FPR rates -- this is so that the model isn't discriminating in error types\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for g in groups:\n",
    "        mask = (sensitive_features == g)\n",
    "        y_true_g = y_true[mask]\n",
    "        y_pred_g = y_pred_bin[mask]\n",
    "        tpr = np.sum((y_pred_g == 1) & (y_true_g == 1)) / (np.sum(y_true_g == 1) + 1e-8) # True Positive Rate\n",
    "        fpr = np.sum((y_pred_g == 1) & (y_true_g == 0)) / (np.sum(y_true_g == 0) + 1e-8) # False Positive Rate\n",
    "        metrics[g] = (tpr, fpr)\n",
    "    eo_diff = (abs(metrics[0][0] - metrics[1][0]) + abs(metrics[0][1] - metrics[1][1])) # taking average of two error types\n",
    "\n",
    "    # Selection rate per group.\n",
    "    \"\"\"\n",
    "    proportion of samples predicted as positive for each group -- a a group has a higher selection rate, the model may favor that group unfairly\n",
    "    \"\"\"\n",
    "    sel_rate = {}\n",
    "    for g in groups:\n",
    "        sel_rate[g] = pos_rates[g]\n",
    "\n",
    "    # Group-wise accuracy.\n",
    "    \"\"\"\n",
    "    for each group in the sensitive feature, compute the accuracy of the model (to ensure that it's perfoming consistently across groups)\n",
    "    \"\"\"\n",
    "    group_acc = {}\n",
    "    for g in groups:\n",
    "        mask = (sensitive_features == g)\n",
    "        group_acc[g] = accuracy_score(y_true[mask], y_pred_bin[mask])\n",
    "\n",
    "    return {\n",
    "        \"demographic_parity_difference\": dp_diff,\n",
    "        \"equalized_odds_difference\": eo_diff,\n",
    "        \"selection_rate\": sel_rate,\n",
    "        \"group_accuracy\": group_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215ef3cd-b252-45a1-a259-d6b4dd09001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Plotting Function\n",
    "# -------------------------------\n",
    "def plot_comparison(metrics_baseline, metrics_fair):\n",
    "    \"\"\"\n",
    "    parameters are dictionaries with the stored values of the evaluation metrics\n",
    "    \"\"\"\n",
    "    models = ['Baseline', 'Fair']\n",
    "    aucs = [metrics_baseline['auc'], metrics_fair['auc']]\n",
    "    accs = [metrics_baseline['accuracy'], metrics_fair['accuracy']]\n",
    "    dp_diff = [metrics_baseline[\"demographic_parity_difference\"], metrics_fair[\"demographic_parity_difference\"]]\n",
    "    eo_diff = [metrics_baseline[\"equalized_odds_difference\"], metrics_fair[\"equalized_odds_difference\"]]\n",
    "\n",
    "    # creating a 2x3 gird of bar chars comparing baseline model and fair model across: AUC, accuracy, demographic parity diff, equalized odd difference\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    ## measures how well the model seperates postiive and negative classes, higher AUC = better model performance\n",
    "    # if fair model has a lower AUC than the baseline, can indicate a fairness-performance tradeoff (meaning less well seperation for more fair results)\n",
    "    axs[0,0].bar(models, aucs, color=['blue', 'green'])\n",
    "    axs[0,0].set_title('AUC')\n",
    "    axs[0,0].set_ylim([0, 1])\n",
    "\n",
    "    ## correct pred/total pred\n",
    "    ## fairness may lower accuracy \n",
    "    axs[0,1].bar(models, accs, color=['blue', 'green'])\n",
    "    axs[0,1].set_title('Accuracy')\n",
    "    axs[0,1].set_ylim([0, 1])\n",
    "\n",
    "    ## orange = baseline, purple = fairness -LOOK INTO TO SEE HOW TO KNOW WHICH GROUP IS CONTRIBUTING TO HIGHER DP\n",
    "    # lower values of dp indciate better fairness\n",
    "    axs[1,0].bar(models, dp_diff, color=['orange', 'purple'])\n",
    "    axs[1,0].set_title('Demographic Parity Difference')\n",
    "\n",
    "    ## lower value - better fairness\n",
    "    ## equalized odds is satisfied if tpr and fpr are equal across the different groups in the sensitive feature\n",
    "    axs[1,1].bar(models, eo_diff, color=['orange', 'purple'])\n",
    "    axs[1,1].set_title('Equalized Odds Difference')\n",
    "\n",
    "    plt.suptitle(\"Comparison: Baseline (X → Y) vs. Fair (X → Y') Model\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb64d7f-b1bf-44e6-8a2e-82ec01d85ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=5000, n_features=30, bias_factor=0.3, noise_level=0.1, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate Sensitive Attribute S ~ Binomial(1, 0.5)\n",
    "    S = np.random.binomial(1, 0.5, size=n_samples)\n",
    "\n",
    "    # Generate Features X: Some function of S + Multinomial noise\n",
    "    X = np.random.randn(n_samples, n_features) + 0.5 * S[:, np.newaxis]\n",
    "\n",
    "    # Generate True Labels Y (linear function of X + noise)\n",
    "    true_weights = np.random.randn(n_features)\n",
    "    Y_continuous = X @ true_weights + np.random.normal(0, noise_level, size=n_samples)\n",
    "\n",
    "    # Convert Y into discrete categories (multi-class setting)\n",
    "    Y = np.digitize(Y_continuous, bins=np.percentile(Y_continuous, [50]))  # 2 classes (0,1) ## change this later if we want to see \n",
    "\n",
    "    X_train, X_test, Y_train_obs, Y_test_obs, S_train, S_test = train_test_split(\n",
    "        X, Y, S, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, Y_train_obs, Y_test_obs, S_train, S_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db287ff-63dd-4c5d-9aa9-324e31b26826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inject_bias(X_train, Y_train, S_train, X_test, Y_test, S_test, bias_factor=0.3, seed=42):\n",
    "def inject_bias(bias_factor=0.4, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    X_train, X_test, Y_train_raw, Y_test_raw, S_train, S_test = generate_synthetic_data()\n",
    "    def apply_bias(Y, S):\n",
    "        flip_mask = np.random.rand(len(Y)) < bias_factor  # Generate a flip mask for this dataset\n",
    "        Y_biased = Y.copy()\n",
    "        Y_biased[flip_mask & (S == 1)] = 1  # Favor positive outcomes for S=1\n",
    "        Y_biased[flip_mask & (S == 0)] = 0  # Favor negative outcomes for S=0\n",
    "        return Y_biased\n",
    "\n",
    "    Y_train_biased = apply_bias(Y_train_raw, S_train)\n",
    "    Y_test_biased = apply_bias(Y_test_raw, S_test)\n",
    "\n",
    "    return Y_train_biased, Y_test_biased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fc6588-04a1-4918-bf7c-6f8466a2da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: AUC:0.9996070947462955, Accuracy:0.99, Demographic Parity Difference:0.015438596491228085, Equalized Odds Difference:0.02260242900471713\n"
     ]
    }
   ],
   "source": [
    "def run_biased_logistic(X_train, Y_train_biased_pred, X_test, Y_test_biased_pred, Y_train_raw, Y_test_raw, S_train, S_test): \n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    clf.fit(X_train, Y_train_biased_pred)\n",
    "    preds = clf.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(Y_test_raw, preds)\n",
    "    acc = accuracy_score(Y_test_raw, (preds > 0.5).astype(int))\n",
    "    fairness = compute_fairness_metrics_manual(Y_test_raw, preds, sensitive_features=S_test)\n",
    "    \n",
    "    return auc, acc, fairness\n",
    "\n",
    "def run_unbiased_logistic(): \n",
    "    X_train, X_test, Y_train_raw, Y_test_raw, S_train, S_test = generate_synthetic_data() ##  Y is binary class, S is binary\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    clf.fit(X_train, Y_train_raw)\n",
    "    preds = clf.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(Y_test_raw, preds)\n",
    "    acc = accuracy_score(Y_test_raw, (preds > 0.5).astype(int))\n",
    "    fairness = compute_fairness_metrics_manual(Y_test_raw, preds, sensitive_features=S_test)\n",
    "\n",
    "    dp_diff = fairness[\"demographic_parity_difference\"]\n",
    "    eo_diff = fairness[\"equalized_odds_difference\"]\n",
    "\n",
    "    print (f\"Baseline: AUC:{auc}, Accuracy:{acc}, Demographic Parity Difference:{dp_diff}, Equalized Odds Difference:{eo_diff}\")\n",
    "\n",
    "run_unbiased_logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9934d0d9-03c6-4480-a9d2-90283ba9e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Main Function: Comparison and Visualization\n",
    "# -------------------------------\n",
    "def main_synthetic(lambda_adv=1.0, epochs=64, batch_size=128):\n",
    "    set_seed(42)\n",
    "\n",
    "    X_train, X_test, Y_train_raw, Y_test_raw, S_train, S_test = generate_synthetic_data() ##  Y is binary class, S is binary\n",
    "    # Y_train_biased, Y_test_biased = inject_bias(X_train, Y_train_raw, S_train, Y_test_raw, S_train, S_test, bias_factor=0.3, seed=42)\n",
    "    Y_train_biased, Y_test_biased = inject_bias(bias_factor=0.3, seed=42)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # One-hot encode S for adversarial model training.\n",
    "    S_train_oh = tf.keras.utils.to_categorical(S_train, num_classes=2)\n",
    "    S_test_oh  = tf.keras.utils.to_categorical(S_test, num_classes=2)\n",
    "\n",
    "    ### 1. Train adversarial debiasing model (X → Y' with adversary)\n",
    "    print(\"\\nTraining adversarial model (X → Y' with adversary) ...\")\n",
    "    adv_model = build_adversarial_model(input_dim, lambda_adv=lambda_adv)\n",
    "    Y_train_biased_exp = Y_train_biased.reshape(-1, 1)\n",
    "    Y_test_biased_exp  = Y_test_biased.reshape(-1, 1)\n",
    "    adv_model.fit([X_train, S_train_oh],\n",
    "                  {\"pseudo_Y\": Y_train_biased_exp, \"S_pred\": S_train_oh, \"Y_pred\": Y_train_biased_exp},\n",
    "                  epochs=64, batch_size=128, verbose=1)\n",
    "\n",
    "    # Get predictions \n",
    "    pseudo_Y_train, S_pred, Y_pred_train = adv_model.predict([X_train, S_train_oh]) ## do we want psuedo_Y or Y_pred? psuedo_Y is for complete fairness why pred_Y can be a bit more accurate by keep necessary dependencies\n",
    "    pseudo_Y_test,  S_pred, Y_pred_test = adv_model.predict([X_test, S_test_oh])\n",
    "\n",
    "    # # THIS IS WITH PSUEDO_Y - Professor's way\n",
    "    Y_pred_train_bin = (pseudo_Y_train > 0.5).astype(np.float32)\n",
    "    Y_pred_test_bin  = (pseudo_Y_test > 0.5).astype(np.float32)\n",
    "\n",
    "    # # # THIS IS WITH Y_PRED\n",
    "    # Y_pred_train_bin = (Y_pred_train > 0.5).astype(np.float32)\n",
    "    # Y_pred_test_bin  = (Y_pred_test > 0.5).astype(np.float32)\n",
    "\n",
    "    print(\"\\nPseudo-label statistics (training):\")\n",
    "    for g in np.unique(S_train):\n",
    "        mask = (S_train == g)\n",
    "        print(f\"Group {g} pseudo-positive rate: {np.mean(Y_pred_train_bin[mask]):.4f}\") # average probability of a postive prediction per group -- fairness check to see if both groups receive similar treatment\n",
    "\n",
    "    ### 2. Train baseline logistic regression model on observed Y (X → Y) -- regular logistic regression for baseline for comparison; does not include any fairness constraints\n",
    "    print(\"\\nTraining baseline [BIASED] logistic regression classifier (X → Y)...\")\n",
    "    baseline_auc, baseline_acc, baseline_fairness = run_biased_logistic(X_train, Y_train_biased, X_test, Y_test_biased,  Y_train_raw, Y_test_raw, S_train, S_test)\n",
    "\n",
    "    ### 3. Train fair logistic regression model on pseudo-labels (X → Y') -- using psuedo_Y from the the adv_model, \n",
    "    print(\"\\nTraining fair logistic regression classifier (X → Y') using pseudo-labels...\")\n",
    "    fair_auc, fair_acc, fair_fairness = run_biased_logistic(X_train, Y_pred_train_bin, X_test, Y_pred_test_bin, Y_train_raw, Y_test_raw, S_train, S_test)\n",
    "\n",
    "    # Aggregate metrics for plotting.\n",
    "    metrics_baseline = {\n",
    "        \"auc\": baseline_auc,\n",
    "        \"accuracy\": baseline_acc,\n",
    "        \"demographic_parity_difference\": baseline_fairness[\"demographic_parity_difference\"],\n",
    "        \"equalized_odds_difference\": baseline_fairness[\"equalized_odds_difference\"]\n",
    "    }\n",
    "    metrics_fair = {\n",
    "        \"auc\": fair_auc,\n",
    "        \"accuracy\": fair_acc,\n",
    "        \"demographic_parity_difference\": fair_fairness[\"demographic_parity_difference\"],\n",
    "        \"equalized_odds_difference\": fair_fairness[\"equalized_odds_difference\"]\n",
    "    }\n",
    "\n",
    "    print(\"\\nBaseline Logistic Regression (X → Y) Evaluation:\")\n",
    "    print(f\"AUC: {baseline_auc:.4f}, Accuracy: {baseline_acc:.4f}\")\n",
    "    print(\"Fairness metrics:\", baseline_fairness)\n",
    "\n",
    "    print(\"\\nFair Logistic Regression (X → Y') Evaluation (compared to observed Y):\")\n",
    "    print(f\"AUC: {fair_auc:.4f}, Accuracy: {fair_acc:.4f}\")\n",
    "    print(\"Fairness metrics:\", fair_fairness)\n",
    "\n",
    "    # Plot comparison.\n",
    "    plot_comparison(metrics_baseline, metrics_fair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a7324-619d-470b-9ec7-f1b4bf6d984f",
   "metadata": {},
   "source": [
    "### Application on Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a8cb1-26fd-47ae-ba82-01642fb05249",
   "metadata": {},
   "source": [
    "#### Synthetic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f08737-2fab-439f-aef0-27536aa9cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_synthetic(lambda_adv=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86e898-d71b-42d7-a0d9-ca66501381ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### last thing to do -- need to hypertune \n",
    "### need to do synthetic dataset for a multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f65247ba-a7aa-4e3b-a8fb-a3ef2b7905e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing lambda_adv=1.0, epochs=32, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=32, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=32, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=32, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=32, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=32, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=64, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=64, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=64, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=64, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=64, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=64, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=128, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=128, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=128, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=128, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=1.0, epochs=128, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=1.0, epochs=128, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=32, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=32, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=32, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=32, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=32, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=32, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=64, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=64, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=64, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=64, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=64, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=64, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=128, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=128, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=128, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=128, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=3.0, epochs=128, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=3.0, epochs=128, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=32, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=32, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=32, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=32, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=32, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=32, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=64, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=64, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=64, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=64, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=64, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=64, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=128, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=128, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=128, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=128, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=5.0, epochs=128, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=5.0, epochs=128, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=32, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=32, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=32, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=32, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=32, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=32, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=64, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=64, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=64, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=64, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=64, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=64, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=128, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=128, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=128, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=128, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=7.0, epochs=128, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=7.0, epochs=128, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=32, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=32, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=32, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=32, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=32, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=32, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=64, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=64, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=64, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=64, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=64, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=64, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=128, batch_size=64\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=128, batch_size=64: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=128, batch_size=128\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=128, batch_size=128: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Testing lambda_adv=15.0, epochs=128, batch_size=256\n",
      "  Fold 1: Score=1.5454, Accuracy=0.8546, AUC=0.9373, Demographic Parity Diff=0.2464\n",
      "  Fold 2: Score=1.5993, Accuracy=0.8612, AUC=0.9472, Demographic Parity Diff=0.2091\n",
      "  Fold 3: Score=1.5828, Accuracy=0.8582, AUC=0.9475, Demographic Parity Diff=0.2229\n",
      "  Final (Avg) for lambda_adv=15.0, epochs=128, batch_size=256: Score=1.5758, Accuracy=0.8580, AUC=0.9440, Demographic Parity Diff=0.2262\n",
      "\n",
      "Best Hyperparameters: lambda_adv                  1.000000\n",
      "epochs                     32.000000\n",
      "batch_size                 64.000000\n",
      "score                       1.575845\n",
      "accuracy                    0.858001\n",
      "auc                         0.944000\n",
      "demographic_parity_diff     0.226156\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "class AdversarialModelWrapperFixed(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Fixed Wrapper for Adversarial Model to work with Grid Search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambda_adv=1.0, epochs=64, batch_size=128):\n",
    "        self.lambda_adv = lambda_adv\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y, S):\n",
    "        \"\"\"\n",
    "        Train the adversarial model. S is now passed dynamically per fold.\n",
    "        \"\"\"\n",
    "        y = y.ravel()  # Convert to 1D array\n",
    "        input_dim = X.shape[1]\n",
    "        S_oh = tf.keras.utils.to_categorical(S, num_classes=2)\n",
    "\n",
    "        self.model = build_adversarial_model(input_dim, lambda_adv=self.lambda_adv)\n",
    "        self.model.fit(\n",
    "            [X, S_oh],\n",
    "            {\"pseudo_Y\": y, \"S_pred\": S_oh, \"Y_pred\": y},\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, S):\n",
    "        \"\"\"\n",
    "        Generate predictions from the trained model. S must match X per fold.\n",
    "        \"\"\"\n",
    "        S_oh = tf.keras.utils.to_categorical(S, num_classes=2)\n",
    "        pseudo_Y, S_pred, Y_pred = self.model.predict([X, S_oh], verbose=0)\n",
    "        return (pseudo_Y > 0.5).astype(np.float32)\n",
    "\n",
    "    def score(self, X_train, Y_train_biased_pred, X_test, Y_test_biased_pred, Y_train_raw, Y_test_raw, S_train, S_test, return_metrics=False):\n",
    "        \"\"\"\n",
    "        Compute the optimization score combining AUC, accuracy, and fairness metrics.\n",
    "        \"\"\"\n",
    "        auc, acc, fairness_metrics = run_biased_logistic(X_train, Y_train_biased_pred, X_test, Y_test_biased_pred, Y_train_raw, Y_test_raw, S_train, S_test)  \n",
    "        demographic_parity_diff = abs(fairness_metrics[\"demographic_parity_difference\"])\n",
    "\n",
    "        # Objective function (equal weights for now)\n",
    "        score = auc + acc - demographic_parity_diff\n",
    "\n",
    "        if return_metrics:\n",
    "            return score, acc, auc, demographic_parity_diff\n",
    "        return score\n",
    "\n",
    "\n",
    "# Load synthetic dataset\n",
    "set_seed(42)\n",
    "X_train, X_test, Y_train_raw, Y_test_raw, S_train, S_test = generate_synthetic_data()\n",
    "Y_train_biased, Y_test_biased = inject_bias(bias_factor=0.3, seed=42)\n",
    "\n",
    "# Convert `Y_train_biased` to a 1D array\n",
    "Y_train_biased = Y_train_biased.ravel()\n",
    "\n",
    "param_grid = {\n",
    "    \"lambda_adv\": [1.0, 3.0, 5.0, 7.0, 15.0],\n",
    "    \"epochs\": [32, 64, 128],\n",
    "    \"batch_size\": [64, 128, 256]\n",
    "}\n",
    "\n",
    "# Custom cross-validation\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Perform manual grid search\n",
    "for lambda_adv, epochs, batch_size in product(param_grid[\"lambda_adv\"], param_grid[\"epochs\"], param_grid[\"batch_size\"]):\n",
    "    scores, accuracies, aucs, demographic_parity_diffs = [], [], [], []\n",
    "    \n",
    "    print(f\"\\nTesting lambda_adv={lambda_adv}, epochs={epochs}, batch_size={batch_size}\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, Y_train_biased)):  # Ensure Y_train_biased is used for stratification\n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        Y_train_fold, Y_val_fold = Y_train_biased[train_idx], Y_train_biased[val_idx]\n",
    "        S_train_fold, S_val_fold = S_train[train_idx], S_train[val_idx]\n",
    "\n",
    "        # Train model\n",
    "        model = AdversarialModelWrapperFixed(lambda_adv=lambda_adv, epochs=epochs, batch_size=batch_size)\n",
    "        model.fit(X_train_fold, Y_train_fold, S=S_train_fold)\n",
    "\n",
    "        # Evaluate model (Pass required arguments to `score`)\n",
    "        score, accuracy, auc, demographic_parity_diff = model.score(\n",
    "            X_train_fold, Y_train_fold, \n",
    "            X_val_fold, Y_val_fold, \n",
    "            Y_train_raw[train_idx], Y_train_raw[val_idx],  # ✅ Corrected to use Y_train_raw[val_idx]\n",
    "            S_train_fold, S_val_fold, \n",
    "            return_metrics=True\n",
    "        )\n",
    "\n",
    "        \n",
    "        scores.append(score)\n",
    "        accuracies.append(accuracy)\n",
    "        aucs.append(auc)\n",
    "        demographic_parity_diffs.append(demographic_parity_diff)\n",
    "\n",
    "        # Print results per fold\n",
    "        print(f\"  Fold {fold + 1}: Score={score:.4f}, Accuracy={accuracy:.4f}, AUC={auc:.4f}, Demographic Parity Diff={demographic_parity_diff:.4f}\")\n",
    "\n",
    "    # Store average scores across folds\n",
    "    avg_score = np.mean(scores)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_auc = np.mean(aucs)\n",
    "    avg_demographic_parity_diff = np.mean(demographic_parity_diffs)\n",
    "\n",
    "    results.append({\n",
    "        \"lambda_adv\": lambda_adv,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"score\": avg_score,\n",
    "        \"accuracy\": avg_accuracy,\n",
    "        \"auc\": avg_auc,\n",
    "        \"demographic_parity_diff\": avg_demographic_parity_diff\n",
    "    })\n",
    "\n",
    "    print(f\"  Final (Avg) for lambda_adv={lambda_adv}, epochs={epochs}, batch_size={batch_size}: \"\n",
    "          f\"Score={avg_score:.4f}, Accuracy={avg_accuracy:.4f}, AUC={avg_auc:.4f}, Demographic Parity Diff={avg_demographic_parity_diff:.4f}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find best hyperparameters\n",
    "best_params = results_df.loc[results_df[\"score\"].idxmax()]\n",
    "print(\"\\nBest Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e4102-7834-4fa7-861c-33a025a58553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ebd500-9c2a-4fbc-b363-fd79165e2187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
